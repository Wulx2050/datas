{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5tPoR5BbEuG",
        "colab_type": "code",
        "outputId": "398443c8-fa67-457e-ac11-ad22598d4c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 25s 2us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e92G4JK5bcsK",
        "colab_type": "code",
        "outputId": "6bfcdc79-f3b3-419a-8989-0949a4fcf585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zutT32e4bcvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgz15S8dbcx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'标签字典'\n",
        "label_dict = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4',\n",
        "              5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
        "\n",
        "def plot_images_lables_prediction(images_list, labels_list, prediction_list, # 图像，标签， 预测值\n",
        "                                  index=0,      # 开始指针\n",
        "                                  num=10, num_max=25):    # 设置显示图的数量\n",
        "    \n",
        "    fig = plt.gcf()  #  获取当前图表 get current figure\n",
        "    fig.set_size_inches(32, 32)  #  1 inches = 2.54 cm\n",
        "    if num > num_max:\n",
        "        num = num_max\n",
        "    for i in range(0, num):\n",
        "        ax = plt.subplot(5, 5, i+1)\n",
        "        ax.imshow(images_list[index], cmap='binary')\n",
        "        title = str(index) + ': ' + label_dict[int(labels_list[index])]\n",
        "        if len(prediction_list) > 0:\n",
        "            title += '= >' + str(prediction_list[index])\n",
        "        ax.set_title(title, fontsize=30)\n",
        "        ax.set_xticks([])   # 不显示x坐标轴\n",
        "        ax.set_yticks([])   # 不显示y坐标轴\n",
        "        index += 1\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2ARyisDbc1I",
        "colab_type": "code",
        "outputId": "3a526c3b-2414-401d-b8f2-6cb09884cb87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        }
      },
      "source": [
        "plot_images_lables_prediction(x_train, y_train, [], index=1000, num=10, num_max=25)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABwcAAALCCAYAAAA709x+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h+rqa3uyF75KQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 2304x2304 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A51GBDN_cNw6",
        "colab_type": "text"
      },
      "source": [
        "### 用于全连接网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY148pw_b5fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train = x_train.reshape(60000, 28*28)\n",
        "x_test = x_test.reshape(10000, 28*28)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JinQYhCNcWlj",
        "colab_type": "text"
      },
      "source": [
        "### 用于卷积网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7oei5Ttbc3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFKGq5tbuYl3",
        "colab_type": "text"
      },
      "source": [
        "### 用于循环网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZWSveCruXGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z060U2UXco0y",
        "colab_type": "text"
      },
      "source": [
        "### 全连接网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy_CfXEZbc7I",
        "colab_type": "code",
        "outputId": "b4a06e3f-5306-4a16-bd2d-0391566db46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "from keras import regularizers\n",
        "from keras import models, layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_dim=(28*28)))  # <<------------------------\n",
        "model.add(layers.Dropout(0.5))\n",
        "# keras.layers.Dropout(rate, noise_shape=None, seed=None)\n",
        "\n",
        "model.add(layers.Dense(512, activation='relu', ))  \n",
        "model.add(layers.Dropout(0.5))    # <<------------------------\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))  # <<------------------------\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0k_vvCxbc-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "# optimizer = Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=1e-5)\n",
        "# optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "# optimizer = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "# optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "# model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.compile(optimizer='RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPmvG9ac2Gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_1.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "# callbacks = [model_checkpoint, reduce_lr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8alKkwLdKKZ",
        "colab_type": "code",
        "outputId": "034536e5-93b4-4b39-fff7-58a81cc1d94e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3529
        }
      },
      "source": [
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_split=0.2,\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 4s 87us/step - loss: 0.3640 - acc: 0.8863 - val_loss: 0.1342 - val_acc: 0.9594\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.95942, saving model to mnist_model_1.h5\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 1s 31us/step - loss: 0.1774 - acc: 0.9479 - val_loss: 0.1102 - val_acc: 0.9683\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.95942 to 0.96833, saving model to mnist_model_1.h5\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.1405 - acc: 0.9597 - val_loss: 0.0981 - val_acc: 0.9731\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.96833 to 0.97308, saving model to mnist_model_1.h5\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.1252 - acc: 0.9651 - val_loss: 0.0937 - val_acc: 0.9756\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.97308 to 0.97558, saving model to mnist_model_1.h5\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.1120 - acc: 0.9681 - val_loss: 0.0970 - val_acc: 0.9758\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.97558 to 0.97575, saving model to mnist_model_1.h5\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.1028 - acc: 0.9718 - val_loss: 0.0912 - val_acc: 0.9768\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.97575 to 0.97683, saving model to mnist_model_1.h5\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0955 - acc: 0.9742 - val_loss: 0.0969 - val_acc: 0.9771\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.97683 to 0.97708, saving model to mnist_model_1.h5\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0908 - acc: 0.9750 - val_loss: 0.0954 - val_acc: 0.9772\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.97708 to 0.97725, saving model to mnist_model_1.h5\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0863 - acc: 0.9762 - val_loss: 0.1014 - val_acc: 0.9782\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.97725 to 0.97825, saving model to mnist_model_1.h5\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0843 - acc: 0.9778 - val_loss: 0.1072 - val_acc: 0.9774\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.97825\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0799 - acc: 0.9787 - val_loss: 0.1034 - val_acc: 0.9783\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.97825 to 0.97833, saving model to mnist_model_1.h5\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0790 - acc: 0.9793 - val_loss: 0.1097 - val_acc: 0.9783\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.97833\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0762 - acc: 0.9805 - val_loss: 0.1034 - val_acc: 0.9797\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.97833 to 0.97967, saving model to mnist_model_1.h5\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0717 - acc: 0.9818 - val_loss: 0.1069 - val_acc: 0.9799\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.97967 to 0.97992, saving model to mnist_model_1.h5\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0759 - acc: 0.9810 - val_loss: 0.1098 - val_acc: 0.9788\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.97992\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0729 - acc: 0.9816 - val_loss: 0.1048 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.97992 to 0.98125, saving model to mnist_model_1.h5\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0695 - acc: 0.9834 - val_loss: 0.1129 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.98125\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0720 - acc: 0.9826 - val_loss: 0.1113 - val_acc: 0.9810\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.98125\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0655 - acc: 0.9840 - val_loss: 0.1121 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.98125\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0661 - acc: 0.9837 - val_loss: 0.1093 - val_acc: 0.9798\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.98125\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0681 - acc: 0.9835 - val_loss: 0.1158 - val_acc: 0.9788\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.98125\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0474 - acc: 0.9881 - val_loss: 0.1090 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.98125 to 0.98158, saving model to mnist_model_1.h5\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0416 - acc: 0.9906 - val_loss: 0.1093 - val_acc: 0.9811\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.98158\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0366 - acc: 0.9912 - val_loss: 0.1118 - val_acc: 0.9820\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.98158 to 0.98200, saving model to mnist_model_1.h5\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0385 - acc: 0.9916 - val_loss: 0.1112 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.98200\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0393 - acc: 0.9915 - val_loss: 0.1103 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.98200\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0328 - acc: 0.9920 - val_loss: 0.1134 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.98200\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0333 - acc: 0.9924 - val_loss: 0.1168 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.98200\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0347 - acc: 0.9922 - val_loss: 0.1139 - val_acc: 0.9820\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.98200\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0314 - acc: 0.9928 - val_loss: 0.1139 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.98200 to 0.98225, saving model to mnist_model_1.h5\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0283 - acc: 0.9934 - val_loss: 0.1131 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.98225 to 0.98233, saving model to mnist_model_1.h5\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0256 - acc: 0.9938 - val_loss: 0.1143 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.98233 to 0.98242, saving model to mnist_model_1.h5\n",
            "Epoch 33/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0268 - acc: 0.9938 - val_loss: 0.1137 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00033: val_acc improved from 0.98242 to 0.98250, saving model to mnist_model_1.h5\n",
            "Epoch 34/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0268 - acc: 0.9937 - val_loss: 0.1143 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.98250\n",
            "Epoch 35/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0269 - acc: 0.9934 - val_loss: 0.1145 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.98250 to 0.98275, saving model to mnist_model_1.h5\n",
            "Epoch 36/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0248 - acc: 0.9935 - val_loss: 0.1150 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.98275\n",
            "Epoch 37/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0271 - acc: 0.9935 - val_loss: 0.1145 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.98275\n",
            "Epoch 38/100\n",
            "48000/48000 [==============================] - 1s 31us/step - loss: 0.0247 - acc: 0.9942 - val_loss: 0.1163 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.98275\n",
            "Epoch 39/100\n",
            "48000/48000 [==============================] - 1s 31us/step - loss: 0.0260 - acc: 0.9939 - val_loss: 0.1172 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.98275\n",
            "Epoch 40/100\n",
            "48000/48000 [==============================] - 1s 31us/step - loss: 0.0273 - acc: 0.9936 - val_loss: 0.1157 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.98275\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 41/100\n",
            "48000/48000 [==============================] - 1s 31us/step - loss: 0.0282 - acc: 0.9932 - val_loss: 0.1156 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.98275\n",
            "Epoch 42/100\n",
            "48000/48000 [==============================] - 2s 31us/step - loss: 0.0269 - acc: 0.9934 - val_loss: 0.1156 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.98275\n",
            "Epoch 43/100\n",
            "48000/48000 [==============================] - 1s 31us/step - loss: 0.0270 - acc: 0.9934 - val_loss: 0.1155 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.98275\n",
            "Epoch 44/100\n",
            "48000/48000 [==============================] - 1s 31us/step - loss: 0.0252 - acc: 0.9938 - val_loss: 0.1156 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.98275\n",
            "Epoch 45/100\n",
            "48000/48000 [==============================] - 1s 30us/step - loss: 0.0283 - acc: 0.9935 - val_loss: 0.1155 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.98275\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 00045: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3298128b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6IVkiElgzLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0er-CsDNc7P_",
        "colab_type": "code",
        "outputId": "36c69cea-da83-4eab-84f6-aa15a787ecb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 13us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.10652298009670341, 0.9847]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrITBQBPc7TL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFVOwY0Kc7Ys",
        "colab_type": "code",
        "outputId": "c3c1c160-b335-4a0f-ba98-a24c7d5efb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from keras import regularizers\n",
        "from keras import models, layers\n",
        "\n",
        "# kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01)\n",
        "# keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
        "# model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01)))\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_dim=(28*28),  \n",
        "                       kernel_regularizer=regularizers.l2(1e-6),    # <<------------------------\n",
        "                       activity_regularizer=regularizers.l2(1e-6))) # <<------------------------\n",
        "model.add(layers.Dropout(0.5))\n",
        "# keras.layers.Dropout(rate, noise_shape=None, seed=None)\n",
        "\n",
        "model.add(layers.Dense(512, activation='relu', \n",
        "                       kernel_regularizer=regularizers.l2(1e-6), \n",
        "                       activity_regularizer=regularizers.l2(1e-6)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu9fNQ6oc7WJ",
        "colab_type": "code",
        "outputId": "0e85570f-73ce-4542-964b-a210497d9373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3635
        }
      },
      "source": [
        "model_checkpoint = ModelCheckpoint('mnist_model_2.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "model.compile(optimizer='RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_split=0.2,\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 2s 40us/step - loss: 0.3813 - acc: 0.8904 - val_loss: 0.1475 - val_acc: 0.9625\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.96250, saving model to mnist_model_2.h5\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.1861 - acc: 0.9507 - val_loss: 0.1216 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.96250 to 0.96883, saving model to mnist_model_2.h5\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.1482 - acc: 0.9616 - val_loss: 0.1135 - val_acc: 0.9708\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.96883 to 0.97083, saving model to mnist_model_2.h5\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.1311 - acc: 0.9654 - val_loss: 0.1066 - val_acc: 0.9731\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.97083 to 0.97308, saving model to mnist_model_2.h5\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.1148 - acc: 0.9701 - val_loss: 0.0999 - val_acc: 0.9747\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.97308 to 0.97467, saving model to mnist_model_2.h5\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.1102 - acc: 0.9720 - val_loss: 0.0992 - val_acc: 0.9766\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.97467 to 0.97658, saving model to mnist_model_2.h5\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.1024 - acc: 0.9735 - val_loss: 0.0979 - val_acc: 0.9769\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.97658 to 0.97692, saving model to mnist_model_2.h5\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0983 - acc: 0.9755 - val_loss: 0.0953 - val_acc: 0.9764\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.97692\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 2s 31us/step - loss: 0.0984 - acc: 0.9760 - val_loss: 0.0931 - val_acc: 0.9777\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.97692 to 0.97767, saving model to mnist_model_2.h5\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0937 - acc: 0.9767 - val_loss: 0.0940 - val_acc: 0.9777\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.97767 to 0.97775, saving model to mnist_model_2.h5\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0929 - acc: 0.9774 - val_loss: 0.0975 - val_acc: 0.9780\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.97775 to 0.97800, saving model to mnist_model_2.h5\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0899 - acc: 0.9785 - val_loss: 0.0897 - val_acc: 0.9797\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.97800 to 0.97967, saving model to mnist_model_2.h5\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0879 - acc: 0.9785 - val_loss: 0.0935 - val_acc: 0.9785\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.97967\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0865 - acc: 0.9789 - val_loss: 0.0946 - val_acc: 0.9787\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.97967\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0875 - acc: 0.9789 - val_loss: 0.0879 - val_acc: 0.9791\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.97967\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0872 - acc: 0.9791 - val_loss: 0.0881 - val_acc: 0.9799\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.97967 to 0.97992, saving model to mnist_model_2.h5\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0861 - acc: 0.9798 - val_loss: 0.0898 - val_acc: 0.9794\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.97992\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0833 - acc: 0.9803 - val_loss: 0.0951 - val_acc: 0.9782\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.97992\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0818 - acc: 0.9804 - val_loss: 0.0881 - val_acc: 0.9802\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.97992 to 0.98017, saving model to mnist_model_2.h5\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 1s 31us/step - loss: 0.0845 - acc: 0.9798 - val_loss: 0.0963 - val_acc: 0.9792\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.98017\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0820 - acc: 0.9809 - val_loss: 0.0916 - val_acc: 0.9804\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.98017 to 0.98042, saving model to mnist_model_2.h5\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0836 - acc: 0.9808 - val_loss: 0.0877 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.98042 to 0.98175, saving model to mnist_model_2.h5\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0841 - acc: 0.9807 - val_loss: 0.0854 - val_acc: 0.9797\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.98175\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 2s 31us/step - loss: 0.0814 - acc: 0.9807 - val_loss: 0.0925 - val_acc: 0.9807\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.98175\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0840 - acc: 0.9811 - val_loss: 0.0904 - val_acc: 0.9805\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.98175\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0820 - acc: 0.9805 - val_loss: 0.0906 - val_acc: 0.9806\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.98175\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0801 - acc: 0.9810 - val_loss: 0.0886 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.98175\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0631 - acc: 0.9851 - val_loss: 0.0819 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.98175 to 0.98183, saving model to mnist_model_2.h5\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0567 - acc: 0.9876 - val_loss: 0.0843 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.98183 to 0.98250, saving model to mnist_model_2.h5\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0553 - acc: 0.9871 - val_loss: 0.0829 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.98250\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0538 - acc: 0.9881 - val_loss: 0.0830 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.98250\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0529 - acc: 0.9876 - val_loss: 0.0824 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.98250 to 0.98258, saving model to mnist_model_2.h5\n",
            "Epoch 33/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0532 - acc: 0.9881 - val_loss: 0.0818 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.98258\n",
            "Epoch 34/100\n",
            "48000/48000 [==============================] - 2s 31us/step - loss: 0.0495 - acc: 0.9889 - val_loss: 0.0816 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.98258 to 0.98292, saving model to mnist_model_2.h5\n",
            "Epoch 35/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0520 - acc: 0.9884 - val_loss: 0.0816 - val_acc: 0.9830\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.98292 to 0.98300, saving model to mnist_model_2.h5\n",
            "Epoch 36/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0522 - acc: 0.9880 - val_loss: 0.0823 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.98300\n",
            "Epoch 37/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0482 - acc: 0.9893 - val_loss: 0.0827 - val_acc: 0.9828\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.98300\n",
            "Epoch 38/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0498 - acc: 0.9886 - val_loss: 0.0808 - val_acc: 0.9831\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.98300 to 0.98308, saving model to mnist_model_2.h5\n",
            "Epoch 39/100\n",
            "48000/48000 [==============================] - 2s 31us/step - loss: 0.0496 - acc: 0.9885 - val_loss: 0.0814 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.98308\n",
            "Epoch 40/100\n",
            "48000/48000 [==============================] - 2s 31us/step - loss: 0.0471 - acc: 0.9889 - val_loss: 0.0821 - val_acc: 0.9831\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.98308\n",
            "Epoch 41/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0496 - acc: 0.9893 - val_loss: 0.0812 - val_acc: 0.9830\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.98308\n",
            "Epoch 42/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0514 - acc: 0.9884 - val_loss: 0.0833 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.98308\n",
            "Epoch 43/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0496 - acc: 0.9886 - val_loss: 0.0827 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.98308\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 44/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0464 - acc: 0.9894 - val_loss: 0.0825 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.98308\n",
            "Epoch 45/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0468 - acc: 0.9895 - val_loss: 0.0823 - val_acc: 0.9828\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.98308\n",
            "Epoch 46/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0459 - acc: 0.9896 - val_loss: 0.0822 - val_acc: 0.9831\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.98308\n",
            "Epoch 47/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0457 - acc: 0.9896 - val_loss: 0.0820 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.98308\n",
            "Epoch 48/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0426 - acc: 0.9904 - val_loss: 0.0824 - val_acc: 0.9828\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.98308\n",
            "\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 00048: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f328813c438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GOI5s4si4Vu",
        "colab_type": "code",
        "outputId": "abc5c32a-9165-4eb3-b19a-ee10a335e8c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_2.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 16us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08724314426928759, 0.9823]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urHMqQ8Qlfic",
        "colab_type": "code",
        "outputId": "c83f96e6-f655-4a61-c2e9-623f73414770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "from keras import regularizers\n",
        "from keras import models, layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_dim=(28*28)))\n",
        "\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18PXF7n7lfeG",
        "colab_type": "code",
        "outputId": "2142f4f2-4b69-43fe-e67c-15a475529202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3059
        }
      },
      "source": [
        "model_checkpoint = ModelCheckpoint('mnist_model_3.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "model.compile(optimizer='RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_split=0.2,\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2510 - acc: 0.9223 - val_loss: 0.1067 - val_acc: 0.9677\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.96775, saving model to mnist_model_3.h5\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0905 - acc: 0.9723 - val_loss: 0.0981 - val_acc: 0.9706\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.96775 to 0.97058, saving model to mnist_model_3.h5\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0582 - acc: 0.9821 - val_loss: 0.0947 - val_acc: 0.9737\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.97058 to 0.97367, saving model to mnist_model_3.h5\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0406 - acc: 0.9873 - val_loss: 0.0953 - val_acc: 0.9761\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.97367 to 0.97608, saving model to mnist_model_3.h5\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0302 - acc: 0.9898 - val_loss: 0.1017 - val_acc: 0.9748\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.97608\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0229 - acc: 0.9928 - val_loss: 0.0971 - val_acc: 0.9772\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.97608 to 0.97717, saving model to mnist_model_3.h5\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0176 - acc: 0.9943 - val_loss: 0.1120 - val_acc: 0.9758\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.97717\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0145 - acc: 0.9957 - val_loss: 0.1268 - val_acc: 0.9771\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.97717\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0123 - acc: 0.9962 - val_loss: 0.1274 - val_acc: 0.9779\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.97717 to 0.97792, saving model to mnist_model_3.h5\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.1335 - val_acc: 0.9782\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.97792 to 0.97817, saving model to mnist_model_3.h5\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 1s 27us/step - loss: 0.0088 - acc: 0.9974 - val_loss: 0.1519 - val_acc: 0.9753\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.97817\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 1s 27us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.1554 - val_acc: 0.9775\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.97817\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 1s 27us/step - loss: 0.0085 - acc: 0.9975 - val_loss: 0.1384 - val_acc: 0.9784\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.97817 to 0.97842, saving model to mnist_model_3.h5\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.1492 - val_acc: 0.9794\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.97842 to 0.97942, saving model to mnist_model_3.h5\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 1s 27us/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.1518 - val_acc: 0.9797\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.97942 to 0.97967, saving model to mnist_model_3.h5\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0080 - acc: 0.9978 - val_loss: 0.1670 - val_acc: 0.9767\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.97967\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 1s 29us/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.1547 - val_acc: 0.9804\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.97967 to 0.98042, saving model to mnist_model_3.h5\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.1555 - val_acc: 0.9805\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.98042 to 0.98050, saving model to mnist_model_3.h5\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.1553 - val_acc: 0.9804\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.98050\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.1552 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.98050 to 0.98175, saving model to mnist_model_3.h5\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0049 - acc: 0.9988 - val_loss: 0.1588 - val_acc: 0.9811\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.98175\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 0.1772 - val_acc: 0.9801\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.98175\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.1501 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.98175 to 0.98217, saving model to mnist_model_3.h5\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.1919 - val_acc: 0.9782\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.98217\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 1s 27us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.1711 - val_acc: 0.9811\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.98217\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.1665 - val_acc: 0.9815\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.98217\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.1625 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.98217\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 1s 27us/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.1867 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.98217\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 2.6218e-04 - acc: 0.9999 - val_loss: 0.1594 - val_acc: 0.9832\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.98217 to 0.98317, saving model to mnist_model_3.h5\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 6.3925e-07 - acc: 1.0000 - val_loss: 0.1575 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.98317 to 0.98442, saving model to mnist_model_3.h5\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 1s 27us/step - loss: 1.4870e-07 - acc: 1.0000 - val_loss: 0.1556 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.98442\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 1s 27us/step - loss: 1.2724e-07 - acc: 1.0000 - val_loss: 0.1559 - val_acc: 0.9843\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.98442\n",
            "Epoch 33/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 1.2455e-07 - acc: 1.0000 - val_loss: 0.1558 - val_acc: 0.9842\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.98442\n",
            "Epoch 34/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 1.2288e-07 - acc: 1.0000 - val_loss: 0.1558 - val_acc: 0.9843\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.98442\n",
            "Epoch 35/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 1.2213e-07 - acc: 1.0000 - val_loss: 0.1557 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.98442\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 36/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 1.2152e-07 - acc: 1.0000 - val_loss: 0.1557 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.98442\n",
            "Epoch 37/100\n",
            "48000/48000 [==============================] - 1s 27us/step - loss: 1.2145e-07 - acc: 1.0000 - val_loss: 0.1557 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.98442\n",
            "Epoch 38/100\n",
            "48000/48000 [==============================] - 1s 27us/step - loss: 1.2138e-07 - acc: 1.0000 - val_loss: 0.1557 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.98442\n",
            "Epoch 39/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 1.2130e-07 - acc: 1.0000 - val_loss: 0.1557 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.98442\n",
            "Epoch 40/100\n",
            "48000/48000 [==============================] - 1s 28us/step - loss: 1.2123e-07 - acc: 1.0000 - val_loss: 0.1557 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.98442\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 00040: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f325fd83c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5phhvgNlqPo",
        "colab_type": "code",
        "outputId": "25c7f6ee-1224-46c5-e52b-8695c1f7d743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_3.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 17us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.14357016448011747, 0.985]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HurXk38ZmXyQ",
        "colab_type": "text"
      },
      "source": [
        "### 卷积网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylkx3Fo7m0kB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHGMCCxPm0zI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrDPQWyulqMP",
        "colab_type": "code",
        "outputId": "c8274f23-d30f-4699-b2b7-763cc8856417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "'实例化一个小型的卷积神经网络'\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))   # <<------------------------\n",
        "model.add(layers.MaxPooling2D((2, 2)))                    # <<------------------------\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "'在卷积神经网络上添加分类器'\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               295168    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 390,410\n",
            "Trainable params: 390,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrKn8F2Ymdu8",
        "colab_type": "code",
        "outputId": "2c8477e6-00a6-4773-84a4-3db63609e92c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2519
        }
      },
      "source": [
        "model_checkpoint = ModelCheckpoint('mnist_model_4.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "model.compile(optimizer='RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_split=0.2,\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 6s 122us/step - loss: 0.2250 - acc: 0.9277 - val_loss: 0.0625 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.98225, saving model to mnist_model_4.h5\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0512 - acc: 0.9842 - val_loss: 0.0396 - val_acc: 0.9892\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.98225 to 0.98917, saving model to mnist_model_4.h5\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0327 - acc: 0.9897 - val_loss: 0.0599 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.98917\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0246 - acc: 0.9922 - val_loss: 0.0399 - val_acc: 0.9892\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98917 to 0.98925, saving model to mnist_model_4.h5\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0188 - acc: 0.9938 - val_loss: 0.0355 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98925 to 0.99058, saving model to mnist_model_4.h5\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0140 - acc: 0.9955 - val_loss: 0.0383 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.99058\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0111 - acc: 0.9964 - val_loss: 0.0442 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99058\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0088 - acc: 0.9972 - val_loss: 0.0500 - val_acc: 0.9890\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99058\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0075 - acc: 0.9979 - val_loss: 0.0497 - val_acc: 0.9894\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99058\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0530 - val_acc: 0.9899\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99058\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0411 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.99058 to 0.99275, saving model to mnist_model_4.h5\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 3.4880e-04 - acc: 0.9999 - val_loss: 0.0429 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.99275 to 0.99317, saving model to mnist_model_4.h5\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 1.6596e-04 - acc: 0.9999 - val_loss: 0.0462 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99317\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 1.1138e-04 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99317\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 7.9083e-05 - acc: 1.0000 - val_loss: 0.0530 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99317\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 3.4977e-05 - acc: 1.0000 - val_loss: 0.0576 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99317\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 7.3719e-06 - acc: 1.0000 - val_loss: 0.0540 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99317\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 2.3212e-06 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99317\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 1.0801e-06 - acc: 1.0000 - val_loss: 0.0557 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.99317 to 0.99325, saving model to mnist_model_4.h5\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 1.5604e-06 - acc: 1.0000 - val_loss: 0.0569 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99325\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 4.2766e-07 - acc: 1.0000 - val_loss: 0.0566 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99325\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 6.2634e-07 - acc: 1.0000 - val_loss: 0.0574 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.99325 to 0.99342, saving model to mnist_model_4.h5\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 6.1204e-07 - acc: 1.0000 - val_loss: 0.0580 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99342\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 2.7807e-07 - acc: 1.0000 - val_loss: 0.0582 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99342\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 3.1114e-07 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99342\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 2.2648e-07 - acc: 1.0000 - val_loss: 0.0590 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99342\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 1.7415e-07 - acc: 1.0000 - val_loss: 0.0601 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99342\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 1.6979e-07 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99342\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 1.6873e-07 - acc: 1.0000 - val_loss: 0.0596 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99342\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 1.7147e-07 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99342\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 1.6550e-07 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99342\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 1.6078e-07 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99342\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 00032: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f325f7055f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNsANC4hmdr5",
        "colab_type": "code",
        "outputId": "fd6ef7bb-9b92-43f7-d176-758def7a325c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_4.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 30us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.052499346948605696, 0.9938]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A-ug6WBmdnV",
        "colab_type": "code",
        "outputId": "f0b09f39-2add-42c3-d453-8446e039f707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(BatchNormalization())                    # <<------------------------\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())                    # <<------------------------\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())                    # <<------------------------\n",
        "\n",
        "'在卷积神经网络上添加分类器'\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 3, 3, 128)         512       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               295168    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 391,306\n",
            "Trainable params: 390,858\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3zOzxwBnkNm",
        "colab_type": "code",
        "outputId": "72d076d7-26f7-4c81-b0b2-681e9dbc8fb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3419
        }
      },
      "source": [
        "model_checkpoint = ModelCheckpoint('mnist_model_5.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "model.compile(optimizer='RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_split=0.2,\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 4s 84us/step - loss: 0.1330 - acc: 0.9602 - val_loss: 0.0815 - val_acc: 0.9777\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97767, saving model to mnist_model_5.h5\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.0507 - acc: 0.9858 - val_loss: 0.0562 - val_acc: 0.9865\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97767 to 0.98650, saving model to mnist_model_5.h5\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.0376 - acc: 0.9892 - val_loss: 0.0539 - val_acc: 0.9864\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.98650\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.0298 - acc: 0.9915 - val_loss: 0.0581 - val_acc: 0.9884\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98650 to 0.98842, saving model to mnist_model_5.h5\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 0.0194 - acc: 0.9944 - val_loss: 0.0628 - val_acc: 0.9869\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.98842\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 0.0205 - acc: 0.9943 - val_loss: 0.0623 - val_acc: 0.9851\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.98842\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.0150 - acc: 0.9956 - val_loss: 0.0479 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.98842 to 0.99017, saving model to mnist_model_5.h5\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 0.0136 - acc: 0.9962 - val_loss: 0.0745 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99017\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.0457 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.99017 to 0.99158, saving model to mnist_model_5.h5\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.0127 - acc: 0.9968 - val_loss: 0.0553 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99158\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 0.0092 - acc: 0.9974 - val_loss: 0.0799 - val_acc: 0.9885\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99158\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 0.0102 - acc: 0.9973 - val_loss: 0.0697 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99158\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.0094 - acc: 0.9975 - val_loss: 0.0680 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99158\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.0082 - acc: 0.9979 - val_loss: 0.0541 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99158\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.0498 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.99158 to 0.99275, saving model to mnist_model_5.h5\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0473 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.99275 to 0.99308, saving model to mnist_model_5.h5\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 3s 66us/step - loss: 9.1572e-04 - acc: 0.9998 - val_loss: 0.0457 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.99308 to 0.99333, saving model to mnist_model_5.h5\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 3s 66us/step - loss: 7.5635e-04 - acc: 0.9999 - val_loss: 0.0500 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99333\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 3s 66us/step - loss: 5.8892e-04 - acc: 0.9999 - val_loss: 0.0492 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99333\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 4.7315e-04 - acc: 0.9999 - val_loss: 0.0504 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99333\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 4.8458e-04 - acc: 0.9999 - val_loss: 0.0487 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.99333 to 0.99358, saving model to mnist_model_5.h5\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 3.9888e-04 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99358\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 4.2724e-04 - acc: 0.9999 - val_loss: 0.0517 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99358\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.8878e-04 - acc: 1.0000 - val_loss: 0.0491 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.99358 to 0.99400, saving model to mnist_model_5.h5\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.5681e-04 - acc: 1.0000 - val_loss: 0.0538 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99400\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.9476e-04 - acc: 1.0000 - val_loss: 0.0546 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99400\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.6130e-04 - acc: 1.0000 - val_loss: 0.0538 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99400\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 4.1872e-04 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99400\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.7581e-04 - acc: 1.0000 - val_loss: 0.0559 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99400\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.4402e-04 - acc: 1.0000 - val_loss: 0.0555 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99400\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.4375e-04 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99400\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.8835e-04 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99400\n",
            "Epoch 33/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 3.9045e-04 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99400\n",
            "Epoch 34/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.5028e-04 - acc: 1.0000 - val_loss: 0.0552 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.99400 to 0.99408, saving model to mnist_model_5.h5\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 35/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.3726e-04 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99408\n",
            "Epoch 36/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.3752e-04 - acc: 1.0000 - val_loss: 0.0556 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99408\n",
            "Epoch 37/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.5377e-04 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99408\n",
            "Epoch 38/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.4106e-04 - acc: 1.0000 - val_loss: 0.0552 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99408\n",
            "Epoch 39/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.6764e-04 - acc: 1.0000 - val_loss: 0.0552 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99408\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 40/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.5343e-04 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99408\n",
            "Epoch 41/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.4231e-04 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.99408\n",
            "Epoch 42/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.3958e-04 - acc: 1.0000 - val_loss: 0.0552 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.99408\n",
            "Epoch 43/100\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 3.4649e-04 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.99408\n",
            "Epoch 44/100\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 3.8119e-04 - acc: 0.9999 - val_loss: 0.0552 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.99408\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "Epoch 00044: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f325edf4e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWndXJlKnkKL",
        "colab_type": "code",
        "outputId": "9a153779-c49c-4009-b428-eab60f629e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_5.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 47us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.051875687020049396, 0.994]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81OpAfwBnkHC",
        "colab_type": "code",
        "outputId": "7139f993-cf6c-46c3-dc35-fe158194a489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        }
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(layers.GlobalAveragePooling2D())   # <<------------------------\n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 3, 3, 128)         512       \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 94,858\n",
            "Trainable params: 94,410\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJfngkDqnkDK",
        "colab_type": "code",
        "outputId": "3a3a5fd7-d70b-4c7f-acec-a3b03532da38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2879
        }
      },
      "source": [
        "model_checkpoint = ModelCheckpoint('mnist_model_6.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "model.compile(optimizer='RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_split=0.2,\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 4s 85us/step - loss: 0.1305 - acc: 0.9671 - val_loss: 0.0639 - val_acc: 0.9792\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97917, saving model to mnist_model_6.h5\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 0.0418 - acc: 0.9879 - val_loss: 0.0444 - val_acc: 0.9858\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97917 to 0.98583, saving model to mnist_model_6.h5\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0289 - acc: 0.9912 - val_loss: 0.0501 - val_acc: 0.9836\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.98583\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0216 - acc: 0.9934 - val_loss: 0.0409 - val_acc: 0.9877\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98583 to 0.98775, saving model to mnist_model_6.h5\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0166 - acc: 0.9950 - val_loss: 0.0575 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.98775\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 0.0144 - acc: 0.9957 - val_loss: 0.0290 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98775 to 0.99083, saving model to mnist_model_6.h5\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 0.0109 - acc: 0.9967 - val_loss: 0.0454 - val_acc: 0.9867\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99083\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0086 - acc: 0.9974 - val_loss: 0.0416 - val_acc: 0.9875\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99083\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0075 - acc: 0.9978 - val_loss: 0.0352 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99083\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0361 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99083\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0292 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.99083 to 0.99167, saving model to mnist_model_6.h5\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0331 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99167\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0294 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.99167 to 0.99242, saving model to mnist_model_6.h5\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0410 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99242\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0392 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99242\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0393 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99242\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0351 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99242\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0418 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99242\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0278 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.99242 to 0.99275, saving model to mnist_model_6.h5\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 3.1551e-04 - acc: 1.0000 - val_loss: 0.0274 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.99275 to 0.99392, saving model to mnist_model_6.h5\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 2.4052e-04 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99392\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 1.5802e-04 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.99392 to 0.99400, saving model to mnist_model_6.h5\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 1.5081e-04 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99400\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 1.3416e-04 - acc: 1.0000 - val_loss: 0.0297 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99400\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 1.4174e-04 - acc: 1.0000 - val_loss: 0.0311 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99400\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 9.1501e-05 - acc: 1.0000 - val_loss: 0.0297 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99400\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 9.9407e-05 - acc: 1.0000 - val_loss: 0.0295 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.99400 to 0.99408, saving model to mnist_model_6.h5\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 6.7023e-05 - acc: 1.0000 - val_loss: 0.0301 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99408\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 5.4577e-05 - acc: 1.0000 - val_loss: 0.0297 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99408\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 5.5857e-05 - acc: 1.0000 - val_loss: 0.0296 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99408\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 4.4689e-05 - acc: 1.0000 - val_loss: 0.0295 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99408\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 5.5778e-05 - acc: 1.0000 - val_loss: 0.0294 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99408\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 33/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 4.3870e-05 - acc: 1.0000 - val_loss: 0.0294 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99408\n",
            "Epoch 34/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 3.4387e-05 - acc: 1.0000 - val_loss: 0.0296 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99408\n",
            "Epoch 35/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 3.7596e-05 - acc: 1.0000 - val_loss: 0.0296 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99408\n",
            "Epoch 36/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 3.8364e-05 - acc: 1.0000 - val_loss: 0.0295 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99408\n",
            "Epoch 37/100\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 8.9498e-05 - acc: 1.0000 - val_loss: 0.0296 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99408\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 00037: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f322f5b1ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s3rEB_Xop08",
        "colab_type": "code",
        "outputId": "5bc4edaf-9538-41a6-9fad-ab39997917a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_6.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 52us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.025630088351408654, 0.9943]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIHqsMctlqHH",
        "colab_type": "code",
        "outputId": "0a0298ca-d940-4c4b-afac-b86dbb39fa45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 7, 7, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 7, 7, 256)         1024      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,913,834\n",
            "Trainable params: 1,912,874\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvTjQB1zprDI",
        "colab_type": "code",
        "outputId": "fd695b11-ee97-4529-9ae6-460ea2c4842f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3151
        }
      },
      "source": [
        "model_checkpoint = ModelCheckpoint('mnist_model_7.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "model.compile(optimizer='RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_split=0.2,\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 12s 259us/step - loss: 0.1781 - acc: 0.9432 - val_loss: 0.3481 - val_acc: 0.9125\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91250, saving model to mnist_model_7.h5\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0475 - acc: 0.9855 - val_loss: 0.0677 - val_acc: 0.9789\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.91250 to 0.97892, saving model to mnist_model_7.h5\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0324 - acc: 0.9902 - val_loss: 0.1055 - val_acc: 0.9677\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.97892\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0259 - acc: 0.9920 - val_loss: 0.0635 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.97892 to 0.98167, saving model to mnist_model_7.h5\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 10s 215us/step - loss: 0.0182 - acc: 0.9943 - val_loss: 0.0493 - val_acc: 0.9849\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98167 to 0.98492, saving model to mnist_model_7.h5\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0175 - acc: 0.9946 - val_loss: 0.0345 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98492 to 0.99092, saving model to mnist_model_7.h5\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0150 - acc: 0.9952 - val_loss: 0.0479 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99092\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0513 - val_acc: 0.9893\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99092\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0097 - acc: 0.9966 - val_loss: 0.0403 - val_acc: 0.9904\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99092\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 10s 216us/step - loss: 0.0081 - acc: 0.9972 - val_loss: 0.0470 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99092\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 10s 215us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0385 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.99092 to 0.99108, saving model to mnist_model_7.h5\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0326 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.99108 to 0.99275, saving model to mnist_model_7.h5\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0334 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99275\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 10s 214us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.0319 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.99275 to 0.99317, saving model to mnist_model_7.h5\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.0408 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99317\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0454 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99317\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.0458 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99317\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0441 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99317\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 10s 212us/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.0466 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99317\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 7.0828e-04 - acc: 0.9998 - val_loss: 0.0303 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.99317 to 0.99467, saving model to mnist_model_7.h5\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 10s 214us/step - loss: 1.3663e-04 - acc: 1.0000 - val_loss: 0.0315 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.99467 to 0.99483, saving model to mnist_model_7.h5\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 7.5532e-05 - acc: 1.0000 - val_loss: 0.0332 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.99483 to 0.99517, saving model to mnist_model_7.h5\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 7.2990e-06 - acc: 1.0000 - val_loss: 0.0320 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.99517 to 0.99533, saving model to mnist_model_7.h5\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 10s 212us/step - loss: 3.4255e-06 - acc: 1.0000 - val_loss: 0.0319 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99533\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 10s 214us/step - loss: 3.2555e-06 - acc: 1.0000 - val_loss: 0.0332 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99533\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 2.7122e-06 - acc: 1.0000 - val_loss: 0.0354 - val_acc: 0.9951\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99533\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 1.2220e-06 - acc: 1.0000 - val_loss: 0.0356 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.99533 to 0.99542, saving model to mnist_model_7.h5\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 7.8265e-07 - acc: 1.0000 - val_loss: 0.0353 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.99542 to 0.99550, saving model to mnist_model_7.h5\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 3.6265e-07 - acc: 1.0000 - val_loss: 0.0355 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99550\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 10s 212us/step - loss: 3.1373e-07 - acc: 1.0000 - val_loss: 0.0366 - val_acc: 0.9950\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99550\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 2.3836e-07 - acc: 1.0000 - val_loss: 0.0360 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.99550 to 0.99567, saving model to mnist_model_7.h5\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 10s 212us/step - loss: 2.4304e-07 - acc: 1.0000 - val_loss: 0.0361 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99567\n",
            "Epoch 33/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 2.9862e-07 - acc: 1.0000 - val_loss: 0.0363 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99567\n",
            "Epoch 34/100\n",
            "48000/48000 [==============================] - 10s 212us/step - loss: 1.8116e-07 - acc: 1.0000 - val_loss: 0.0366 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99567\n",
            "Epoch 35/100\n",
            "48000/48000 [==============================] - 10s 212us/step - loss: 4.0627e-07 - acc: 1.0000 - val_loss: 0.0355 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99567\n",
            "Epoch 36/100\n",
            "48000/48000 [==============================] - 10s 212us/step - loss: 1.8170e-07 - acc: 1.0000 - val_loss: 0.0363 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99567\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 37/100\n",
            "48000/48000 [==============================] - 10s 212us/step - loss: 1.5212e-07 - acc: 1.0000 - val_loss: 0.0364 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99567\n",
            "Epoch 38/100\n",
            "48000/48000 [==============================] - 10s 212us/step - loss: 1.5446e-07 - acc: 1.0000 - val_loss: 0.0365 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99567\n",
            "Epoch 39/100\n",
            "48000/48000 [==============================] - 10s 213us/step - loss: 2.0646e-07 - acc: 1.0000 - val_loss: 0.0366 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99567\n",
            "Epoch 40/100\n",
            "48000/48000 [==============================] - 10s 212us/step - loss: 1.8319e-07 - acc: 1.0000 - val_loss: 0.0368 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99567\n",
            "Epoch 41/100\n",
            "48000/48000 [==============================] - 10s 214us/step - loss: 1.6079e-07 - acc: 1.0000 - val_loss: 0.0366 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.99567\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 00041: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f322ef7c160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz6UwmeCprJH",
        "colab_type": "code",
        "outputId": "2b582642-8f89-4a55-ccc1-3164c04432df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_7.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 144us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03298757810119462, 0.9947]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HuTgPO4prPM",
        "colab_type": "code",
        "outputId": "6e522525-ef68-4bba-9c5a-6e675dfa4b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_20 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 7, 7, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 7, 7, 256)         1024      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,913,834\n",
            "Trainable params: 1,912,874\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erNhVZ6LprVY",
        "colab_type": "code",
        "outputId": "6a3c87e9-e233-4c89-b151-95af97da4913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2611
        }
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_8.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "# optimizer = Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=1e-5)\n",
        "# optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "# optimizer = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)   # <<------------------------\n",
        "\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.compile(optimizer='RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.1631 - acc: 0.9492 - val_loss: 0.5061 - val_acc: 0.8995\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.89950, saving model to mnist_model_8.h5\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 0.0432 - acc: 0.9869 - val_loss: 0.0320 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.89950 to 0.99050, saving model to mnist_model_8.h5\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0295 - acc: 0.9906 - val_loss: 0.0596 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.99050\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0227 - acc: 0.9930 - val_loss: 0.0459 - val_acc: 0.9894\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.99050\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0360 - val_acc: 0.9899\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.99050\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0367 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.99050\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0124 - acc: 0.9963 - val_loss: 0.0263 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.99050 to 0.99270, saving model to mnist_model_8.h5\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0110 - acc: 0.9967 - val_loss: 0.0424 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99270\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0088 - acc: 0.9973 - val_loss: 0.0253 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.99270 to 0.99320, saving model to mnist_model_8.h5\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.0235 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.99320 to 0.99370, saving model to mnist_model_8.h5\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.0321 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99370\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0063 - acc: 0.9978 - val_loss: 0.0281 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99370\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0321 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99370\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0299 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99370\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0404 - val_acc: 0.9907\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99370\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 13s 208us/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0240 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.99370 to 0.99520, saving model to mnist_model_8.h5\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 13s 208us/step - loss: 8.2435e-05 - acc: 1.0000 - val_loss: 0.0272 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99520\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 4.9461e-05 - acc: 1.0000 - val_loss: 0.0277 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99520\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 1.1918e-05 - acc: 1.0000 - val_loss: 0.0275 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.99520 to 0.99530, saving model to mnist_model_8.h5\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 1.7573e-05 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99530\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 4.2267e-06 - acc: 1.0000 - val_loss: 0.0293 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99530\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 2.0258e-06 - acc: 1.0000 - val_loss: 0.0298 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99530\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 1.5730e-06 - acc: 1.0000 - val_loss: 0.0301 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.99530 to 0.99540, saving model to mnist_model_8.h5\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 1.0521e-06 - acc: 1.0000 - val_loss: 0.0303 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99540\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 1.6331e-06 - acc: 1.0000 - val_loss: 0.0301 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99540\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 5.7773e-07 - acc: 1.0000 - val_loss: 0.0309 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99540\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 6.0204e-07 - acc: 1.0000 - val_loss: 0.0310 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99540\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 5.5442e-07 - acc: 1.0000 - val_loss: 0.0312 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99540\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 5.3839e-07 - acc: 1.0000 - val_loss: 0.0310 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99540\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 4.5099e-07 - acc: 1.0000 - val_loss: 0.0309 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99540\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 4.4846e-07 - acc: 1.0000 - val_loss: 0.0309 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99540\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 8.7474e-07 - acc: 1.0000 - val_loss: 0.0309 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99540\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 4.2706e-07 - acc: 1.0000 - val_loss: 0.0308 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99540\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 00033: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f322ef7c198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae_blRMBsgQ0",
        "colab_type": "code",
        "outputId": "4f3315bc-82c8-4591-cc16-0d2e4f8f9898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_8.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 134us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.030105738508643502, 0.9954]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mJgpQTSsgKG",
        "colab_type": "code",
        "outputId": "3cbd44ef-afa2-473d-e139-d076184cfe6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 7, 7, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 7, 7, 256)         1024      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,913,834\n",
            "Trainable params: 1,912,874\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr_vqJ3Gpq92",
        "colab_type": "code",
        "outputId": "dcb2ef0c-ad72-4086-e95d-29bc9f27714f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2827
        }
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_9.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "# optimizer = Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=1e-5)\n",
        "# optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "# optimizer = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.compile(optimizer='RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_data=(x_test, y_test),  # <<------------------------\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 15s 253us/step - loss: 0.1530 - acc: 0.9525 - val_loss: 0.1166 - val_acc: 0.9657\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.96570, saving model to mnist_model_9.h5\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 0.0415 - acc: 0.9875 - val_loss: 0.1235 - val_acc: 0.9654\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.96570\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0310 - acc: 0.9903 - val_loss: 0.0524 - val_acc: 0.9836\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.96570 to 0.98360, saving model to mnist_model_9.h5\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0223 - acc: 0.9931 - val_loss: 0.0765 - val_acc: 0.9782\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.98360\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0185 - acc: 0.9942 - val_loss: 0.0346 - val_acc: 0.9890\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98360 to 0.98900, saving model to mnist_model_9.h5\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0158 - acc: 0.9954 - val_loss: 0.0452 - val_acc: 0.9892\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98900 to 0.98920, saving model to mnist_model_9.h5\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0127 - acc: 0.9959 - val_loss: 0.0606 - val_acc: 0.9851\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98920\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0443 - val_acc: 0.9885\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98920\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0266 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.98920 to 0.99370, saving model to mnist_model_9.h5\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0385 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99370\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0074 - acc: 0.9979 - val_loss: 0.0256 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.99370 to 0.99430, saving model to mnist_model_9.h5\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0250 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.99430 to 0.99460, saving model to mnist_model_9.h5\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0388 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99460\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.0485 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99460\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0049 - acc: 0.9985 - val_loss: 0.0322 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99460\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0360 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99460\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0318 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99460\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 7.2943e-04 - acc: 0.9998 - val_loss: 0.0196 - val_acc: 0.9956\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.99460 to 0.99560, saving model to mnist_model_9.h5\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 1.1120e-04 - acc: 1.0000 - val_loss: 0.0229 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99560\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 1.0499e-04 - acc: 1.0000 - val_loss: 0.0242 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99560\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 2.2710e-05 - acc: 1.0000 - val_loss: 0.0228 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.99560 to 0.99630, saving model to mnist_model_9.h5\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 1.2672e-05 - acc: 1.0000 - val_loss: 0.0253 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99630\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 2.7324e-06 - acc: 1.0000 - val_loss: 0.0256 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99630\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 6.1413e-06 - acc: 1.0000 - val_loss: 0.0254 - val_acc: 0.9960\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99630\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 8.8841e-07 - acc: 1.0000 - val_loss: 0.0278 - val_acc: 0.9956\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99630\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 4.4341e-06 - acc: 1.0000 - val_loss: 0.0273 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.99630 to 0.99640, saving model to mnist_model_9.h5\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 6.9039e-07 - acc: 1.0000 - val_loss: 0.0273 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99640\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 4.1375e-07 - acc: 1.0000 - val_loss: 0.0275 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99640\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 3.4278e-07 - acc: 1.0000 - val_loss: 0.0275 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99640\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 3.7598e-07 - acc: 1.0000 - val_loss: 0.0277 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99640\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 4.6893e-07 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99640\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 3.6108e-07 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99640\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 9.9566e-07 - acc: 1.0000 - val_loss: 0.0278 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99640\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 13s 212us/step - loss: 3.2843e-07 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9960\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99640\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 8.2730e-07 - acc: 1.0000 - val_loss: 0.0278 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99640\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 2.7486e-07 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9960\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99640\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 00036: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f325f763a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owKUriwUvh66",
        "colab_type": "code",
        "outputId": "73a9caea-0cd8-456d-eee6-7c76cf95d3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_9.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 151us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.027285953699133596, 0.9964]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyV9T6NOwffj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzJ7tzFtwftr",
        "colab_type": "code",
        "outputId": "43d75a20-bcfc-433d-aebb-b261ed475a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_40 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 7, 7, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 7, 7, 256)         1024      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_5 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,913,834\n",
            "Trainable params: 1,912,874\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxV6T78Wwf1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # 在数据集上将输入平均值设置为0\n",
        "        samplewise_center=False,  # 将每个样本均值设置为0\n",
        "        featurewise_std_normalization=False,  # 将输入除以数据集的std\n",
        "        samplewise_std_normalization=False,  # 将每个输入除以其std\n",
        "        zca_whitening=False,  # 应用ZCA白化\n",
        "        rotation_range=10,  # 在一个范围下随机旋转图像(degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # 水平随机移位图像（总宽度的分数）\n",
        "        height_shift_range=0.1,  # 随机地垂直移动图像（总高度的分数）\n",
        "        horizontal_flip=False,  # 随机翻转图像\n",
        "        vertical_flip=False)  # 随机翻转图像\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0mENMHzwgA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_10.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "# optimizer = Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=1e-5)\n",
        "# optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "# optimizer = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPpxmqv5wgMK",
        "colab_type": "code",
        "outputId": "b65ae02b-501c-4af2-8847-f1b2b2f50523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2521
        }
      },
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                        batch_size=128),\n",
        "                        samples_per_epoch=x_train.shape[0],\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=468, epochs=100)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "468/468 [==============================] - 20s 43ms/step - loss: 0.1204 - acc: 0.9626 - val_loss: 0.0716 - val_acc: 0.9801\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.98010, saving model to mnist_model_10.h5\n",
            "Epoch 2/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0497 - acc: 0.9846 - val_loss: 0.0477 - val_acc: 0.9850\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.98010 to 0.98500, saving model to mnist_model_10.h5\n",
            "Epoch 3/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0372 - acc: 0.9892 - val_loss: 0.0339 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.98500 to 0.99030, saving model to mnist_model_10.h5\n",
            "Epoch 4/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0352 - acc: 0.9892 - val_loss: 0.1573 - val_acc: 0.9553\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.99030\n",
            "Epoch 5/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0287 - acc: 0.9911 - val_loss: 0.0207 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.99030 to 0.99370, saving model to mnist_model_10.h5\n",
            "Epoch 6/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0285 - acc: 0.9917 - val_loss: 0.0589 - val_acc: 0.9804\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.99370\n",
            "Epoch 7/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0262 - acc: 0.9919 - val_loss: 0.0231 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99370\n",
            "Epoch 8/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0229 - acc: 0.9932 - val_loss: 0.0219 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99370\n",
            "Epoch 9/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0233 - acc: 0.9930 - val_loss: 0.0587 - val_acc: 0.9809\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99370\n",
            "Epoch 10/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0199 - acc: 0.9937 - val_loss: 0.0354 - val_acc: 0.9891\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99370\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 11/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0108 - acc: 0.9966 - val_loss: 0.0125 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.99370 to 0.99570, saving model to mnist_model_10.h5\n",
            "Epoch 12/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0116 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.99570 to 0.99630, saving model to mnist_model_10.h5\n",
            "Epoch 13/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.0119 - val_acc: 0.9966\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.99630 to 0.99660, saving model to mnist_model_10.h5\n",
            "Epoch 14/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0073 - acc: 0.9978 - val_loss: 0.0134 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99660\n",
            "Epoch 15/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.0133 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99660\n",
            "Epoch 16/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0073 - acc: 0.9977 - val_loss: 0.0118 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99660\n",
            "Epoch 17/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0138 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99660\n",
            "Epoch 18/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0067 - acc: 0.9981 - val_loss: 0.0134 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99660\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 19/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0052 - acc: 0.9983 - val_loss: 0.0110 - val_acc: 0.9960\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99660\n",
            "Epoch 20/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0105 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99660\n",
            "Epoch 21/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0103 - val_acc: 0.9968\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.99660 to 0.99680, saving model to mnist_model_10.h5\n",
            "Epoch 22/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0098 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99680\n",
            "Epoch 23/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0108 - val_acc: 0.9968\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99680\n",
            "Epoch 24/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.0107 - val_acc: 0.9966\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99680\n",
            "Epoch 25/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0119 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99680\n",
            "Epoch 26/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0119 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99680\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 27/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0114 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99680\n",
            "Epoch 28/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0111 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99680\n",
            "Epoch 29/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0111 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99680\n",
            "Epoch 30/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0109 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99680\n",
            "Epoch 31/100\n",
            "468/468 [==============================] - 16s 35ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0107 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99680\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 00031: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f322d59cd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVUnG6pSwgUR",
        "colab_type": "code",
        "outputId": "2b3ff2d0-7b44-4fee-da71-a0ad3135d42e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_10.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 178us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.010309792249285965, 0.9968]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLTfsvK4wghY",
        "colab_type": "code",
        "outputId": "31a52db2-82ed-4378-e73d-6e6e1f6ce8d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1259
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization, concatenate\n",
        "\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "temp = inputs\n",
        "x = Conv2D(16, (3, 3), padding='same', activation='relu')(inputs)\n",
        "x = Conv2D(16, (3, 3), padding='same', activation='relu')(x)\n",
        "x = concatenate([x, temp])\n",
        "x = BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "temp = x\n",
        "x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
        "x = concatenate([x, temp])\n",
        "x = BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "temp = x\n",
        "x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "x = concatenate([x, temp])\n",
        "x = BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "temp = x\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = concatenate([x, temp])\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "x = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 28, 28, 16)   160         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 28, 28, 16)   2320        conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 28, 28, 17)   0           conv2d_51[0][0]                  \n",
            "                                                                 input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 28, 28, 17)   68          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 14, 14, 17)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 14, 14, 32)   4928        max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 14, 14, 32)   9248        conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 14, 14, 32)   9248        conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 14, 14, 49)   0           conv2d_54[0][0]                  \n",
            "                                                                 max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 14, 14, 49)   196         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling2D) (None, 7, 7, 49)     0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 7, 7, 64)     28288       max_pooling2d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 7, 7, 113)    0           conv2d_58[0][0]                  \n",
            "                                                                 max_pooling2d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 7, 7, 113)    452         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling2D) (None, 3, 3, 113)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 3, 3, 128)    130304      max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 3, 3, 128)    147584      conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 3, 3, 128)    147584      conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 3, 3, 128)    147584      conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 3, 3, 128)    147584      conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 3, 3, 241)    0           conv2d_63[0][0]                  \n",
            "                                                                 max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 3, 3, 241)    964         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_6 (Glo (None, 241)          0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 241)          0           global_average_pooling2d_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 10)           2420        dropout_11[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 889,716\n",
            "Trainable params: 888,876\n",
            "Non-trainable params: 840\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyNctz0kwgwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # 在数据集上将输入平均值设置为0\n",
        "        samplewise_center=False,  # 将每个样本均值设置为0\n",
        "        featurewise_std_normalization=False,  # 将输入除以数据集的std\n",
        "        samplewise_std_normalization=False,  # 将每个输入除以其std\n",
        "        zca_whitening=False,  # 应用ZCA白化\n",
        "        rotation_range=10,  # 在一个范围下随机旋转图像(degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # 水平随机移位图像（总宽度的分数）\n",
        "        height_shift_range=0.1,  # 随机地垂直移动图像（总高度的分数）\n",
        "        horizontal_flip=False,  # 随机翻转图像\n",
        "        vertical_flip=False)  # 随机翻转图像\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlPGKJLi3UnU",
        "colab_type": "code",
        "outputId": "d974e377-ced1-4bb1-a52f-576cae9bc216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2881
        }
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_11.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                        batch_size=128),\n",
        "                        samples_per_epoch=x_train.shape[0],\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=468, epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "468/468 [==============================] - 20s 42ms/step - loss: 0.1826 - acc: 0.9444 - val_loss: 0.3753 - val_acc: 0.8884\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.88840, saving model to mnist_model_11.h5\n",
            "Epoch 2/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0706 - acc: 0.9793 - val_loss: 0.0863 - val_acc: 0.9742\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.88840 to 0.97420, saving model to mnist_model_11.h5\n",
            "Epoch 3/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0511 - acc: 0.9853 - val_loss: 0.0796 - val_acc: 0.9752\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.97420 to 0.97520, saving model to mnist_model_11.h5\n",
            "Epoch 4/100\n",
            "468/468 [==============================] - 15s 31ms/step - loss: 0.0443 - acc: 0.9867 - val_loss: 0.0917 - val_acc: 0.9722\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.97520\n",
            "Epoch 5/100\n",
            "468/468 [==============================] - 15s 31ms/step - loss: 0.0379 - acc: 0.9885 - val_loss: 0.0513 - val_acc: 0.9852\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.97520 to 0.98520, saving model to mnist_model_11.h5\n",
            "Epoch 6/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0355 - acc: 0.9893 - val_loss: 0.0627 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.98520\n",
            "Epoch 7/100\n",
            "468/468 [==============================] - 15s 33ms/step - loss: 0.0305 - acc: 0.9913 - val_loss: 0.0547 - val_acc: 0.9845\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98520\n",
            "Epoch 8/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0296 - acc: 0.9913 - val_loss: 0.0639 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98520\n",
            "Epoch 9/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0290 - acc: 0.9917 - val_loss: 0.0381 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.98520 to 0.98810, saving model to mnist_model_11.h5\n",
            "Epoch 10/100\n",
            "468/468 [==============================] - 15s 31ms/step - loss: 0.0259 - acc: 0.9924 - val_loss: 0.0725 - val_acc: 0.9795\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98810\n",
            "Epoch 11/100\n",
            "468/468 [==============================] - 15s 33ms/step - loss: 0.0252 - acc: 0.9924 - val_loss: 0.0278 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.98810 to 0.99220, saving model to mnist_model_11.h5\n",
            "Epoch 12/100\n",
            "468/468 [==============================] - 15s 31ms/step - loss: 0.0239 - acc: 0.9934 - val_loss: 0.0237 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.99220 to 0.99300, saving model to mnist_model_11.h5\n",
            "Epoch 13/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0230 - acc: 0.9933 - val_loss: 0.0439 - val_acc: 0.9871\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99300\n",
            "Epoch 14/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0197 - acc: 0.9939 - val_loss: 0.0527 - val_acc: 0.9839\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99300\n",
            "Epoch 15/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0220 - acc: 0.9934 - val_loss: 0.0250 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.99300 to 0.99320, saving model to mnist_model_11.h5\n",
            "Epoch 16/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0181 - acc: 0.9946 - val_loss: 0.0264 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99320\n",
            "Epoch 17/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0187 - acc: 0.9944 - val_loss: 0.0305 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99320\n",
            "Epoch 18/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0192 - acc: 0.9943 - val_loss: 0.0272 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99320\n",
            "Epoch 19/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0180 - acc: 0.9950 - val_loss: 0.0232 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.99320 to 0.99360, saving model to mnist_model_11.h5\n",
            "Epoch 20/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0183 - acc: 0.9948 - val_loss: 0.0348 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99360\n",
            "Epoch 21/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0145 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.99360 to 0.99570, saving model to mnist_model_11.h5\n",
            "Epoch 22/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0145 - acc: 0.9956 - val_loss: 0.0252 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99570\n",
            "Epoch 23/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0164 - acc: 0.9951 - val_loss: 0.0261 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99570\n",
            "Epoch 24/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0152 - acc: 0.9957 - val_loss: 0.0231 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99570\n",
            "Epoch 25/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0150 - acc: 0.9956 - val_loss: 0.0288 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99570\n",
            "Epoch 26/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0134 - acc: 0.9960 - val_loss: 0.0234 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99570\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 27/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0086 - acc: 0.9975 - val_loss: 0.0089 - val_acc: 0.9970\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.99570 to 0.99700, saving model to mnist_model_11.h5\n",
            "Epoch 28/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.0104 - val_acc: 0.9967\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99700\n",
            "Epoch 29/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0102 - val_acc: 0.9970\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99700\n",
            "Epoch 30/100\n",
            "468/468 [==============================] - 15s 31ms/step - loss: 0.0062 - acc: 0.9984 - val_loss: 0.0116 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99700\n",
            "Epoch 31/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0123 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99700\n",
            "Epoch 32/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0140 - val_acc: 0.9956\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99700\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 33/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0119 - val_acc: 0.9960\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99700\n",
            "Epoch 34/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0118 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99700\n",
            "Epoch 35/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0116 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99700\n",
            "Epoch 36/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0116 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99700\n",
            "Epoch 37/100\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0122 - val_acc: 0.9960\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99700\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 00037: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f322ef08588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0nQF3C03UxI",
        "colab_type": "code",
        "outputId": "c364931e-80a2-4cc2-bb02-37f0609f50c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_11.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 198us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.008916603633237537, 0.997]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjrmBHK3U8L",
        "colab_type": "code",
        "outputId": "61fa9961-cc4e-4c09-ef1b-4b344cdd5b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1385
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization, concatenate\n",
        "\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "\n",
        "x = Conv2D(16, (3, 3), activation='relu')(inputs)\n",
        "x = Conv2D(16, (1, 1), activation='relu')(x)\n",
        "\n",
        "x1 = Conv2D(16, (1, 3), activation='relu')(inputs)\n",
        "x1 = Conv2D(16, (3, 1), activation='relu')(x1)\n",
        "\n",
        "x2 = Conv2D(16, (1, 1), activation='relu')(inputs)\n",
        "x2 = Conv2D(16, (3, 3), activation='relu')(x2)\n",
        "\n",
        "x = concatenate([x, x1, x2])\n",
        "x = BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "\n",
        "temp = x\n",
        "x = Conv2D(32, (3, 3), activation='relu')(temp)\n",
        "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
        "x = Conv2D(32, (1, 1), activation='relu')(x)\n",
        "\n",
        "x1 = Conv2D(32, (1, 3), activation='relu')(temp)\n",
        "x1 = Conv2D(32, (3, 1), activation='relu')(x1)\n",
        "x1 = Conv2D(32, (3, 3), activation='relu')(x1)\n",
        "\n",
        "x2 = Conv2D(32, (1, 1), activation='relu')(temp)\n",
        "x2 = Conv2D(32, (3, 3), activation='relu')(x2)\n",
        "x2 = Conv2D(32, (3, 3), activation='relu')(x2)\n",
        "x = concatenate([x, x1, x2])\n",
        "x = BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "\n",
        "temp = x\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = concatenate([x, temp])\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "x = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 26, 26, 16)   160         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 28, 26, 16)   64          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 28, 28, 16)   32          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 26, 26, 16)   272         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 26, 26, 16)   784         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 26, 26, 16)   2320        conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 26, 26, 48)   0           conv2d_65[0][0]                  \n",
            "                                                                 conv2d_67[0][0]                  \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 26, 26, 48)   192         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling2D) (None, 13, 13, 48)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 11, 11, 32)   13856       max_pooling2d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 13, 11, 32)   4640        max_pooling2d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 13, 13, 32)   1568        max_pooling2d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 9, 9, 32)     9248        conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 11, 11, 32)   3104        conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 11, 11, 32)   9248        conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 9, 9, 32)     1056        conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 9, 9, 32)     9248        conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 9, 9, 32)     9248        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 9, 9, 96)     0           conv2d_72[0][0]                  \n",
            "                                                                 conv2d_75[0][0]                  \n",
            "                                                                 conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 9, 9, 96)     384         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling2D) (None, 4, 4, 96)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 4, 4, 128)    110720      max_pooling2d_19[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 4, 4, 128)    147584      conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 4, 4, 128)    147584      conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 4, 4, 128)    147584      conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 4, 4, 128)    147584      conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 4, 4, 224)    0           conv2d_83[0][0]                  \n",
            "                                                                 max_pooling2d_19[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 4, 4, 224)    896         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_7 (Glo (None, 224)          0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 224)          0           global_average_pooling2d_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 10)           2250        dropout_12[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 769,626\n",
            "Trainable params: 768,890\n",
            "Non-trainable params: 736\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMIHZDCA8Iax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # 在数据集上将输入平均值设置为0\n",
        "        samplewise_center=False,  # 将每个样本均值设置为0\n",
        "        featurewise_std_normalization=False,  # 将输入除以数据集的std\n",
        "        samplewise_std_normalization=False,  # 将每个输入除以其std\n",
        "        zca_whitening=False,  # 应用ZCA白化\n",
        "        rotation_range=10,  # 在一个范围下随机旋转图像(degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # 水平随机移位图像（总宽度的分数）\n",
        "        height_shift_range=0.1,  # 随机地垂直移动图像（总高度的分数）\n",
        "        horizontal_flip=False,  # 随机翻转图像\n",
        "        vertical_flip=False)  # 随机翻转图像\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmlUMalS8IWA",
        "colab_type": "code",
        "outputId": "32f9d255-427a-4e77-dae7-3415a25537b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2197
        }
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_12.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                        batch_size=128),\n",
        "                        samples_per_epoch=x_train.shape[0],\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=468, epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.1749 - acc: 0.9473 - val_loss: 0.1104 - val_acc: 0.9679\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.96790, saving model to mnist_model_12.h5\n",
            "Epoch 2/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0673 - acc: 0.9804 - val_loss: 0.0648 - val_acc: 0.9794\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.96790 to 0.97940, saving model to mnist_model_12.h5\n",
            "Epoch 3/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0501 - acc: 0.9859 - val_loss: 0.1611 - val_acc: 0.9539\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.97940\n",
            "Epoch 4/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0416 - acc: 0.9882 - val_loss: 0.0205 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.97940 to 0.99390, saving model to mnist_model_12.h5\n",
            "Epoch 5/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0378 - acc: 0.9897 - val_loss: 0.0194 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.99390 to 0.99420, saving model to mnist_model_12.h5\n",
            "Epoch 6/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0381 - acc: 0.9895 - val_loss: 0.0474 - val_acc: 0.9864\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.99420\n",
            "Epoch 7/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0302 - acc: 0.9914 - val_loss: 0.0331 - val_acc: 0.9894\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99420\n",
            "Epoch 8/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0308 - acc: 0.9913 - val_loss: 0.0610 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99420\n",
            "Epoch 9/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0281 - acc: 0.9918 - val_loss: 0.0359 - val_acc: 0.9891\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99420\n",
            "Epoch 10/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0262 - acc: 0.9925 - val_loss: 0.0396 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99420\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 11/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0156 - acc: 0.9958 - val_loss: 0.0149 - val_acc: 0.9958\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.99420 to 0.99580, saving model to mnist_model_12.h5\n",
            "Epoch 12/100\n",
            "468/468 [==============================] - 16s 33ms/step - loss: 0.0122 - acc: 0.9965 - val_loss: 0.0155 - val_acc: 0.9958\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99580\n",
            "Epoch 13/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0109 - acc: 0.9969 - val_loss: 0.0186 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99580\n",
            "Epoch 14/100\n",
            "468/468 [==============================] - 16s 33ms/step - loss: 0.0108 - acc: 0.9971 - val_loss: 0.0191 - val_acc: 0.9950\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99580\n",
            "Epoch 15/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0107 - acc: 0.9968 - val_loss: 0.0218 - val_acc: 0.9944\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99580\n",
            "Epoch 16/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0183 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99580\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 17/100\n",
            "468/468 [==============================] - 16s 33ms/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0140 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.99580 to 0.99620, saving model to mnist_model_12.h5\n",
            "Epoch 18/100\n",
            "468/468 [==============================] - 16s 33ms/step - loss: 0.0077 - acc: 0.9979 - val_loss: 0.0138 - val_acc: 0.9958\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99620\n",
            "Epoch 19/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.0129 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99620\n",
            "Epoch 20/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0069 - acc: 0.9981 - val_loss: 0.0143 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99620\n",
            "Epoch 21/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.0154 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99620\n",
            "Epoch 22/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0070 - acc: 0.9981 - val_loss: 0.0143 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99620\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 23/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0146 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99620\n",
            "Epoch 24/100\n",
            "468/468 [==============================] - 16s 33ms/step - loss: 0.0062 - acc: 0.9983 - val_loss: 0.0146 - val_acc: 0.9960\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99620\n",
            "Epoch 25/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0056 - acc: 0.9985 - val_loss: 0.0145 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99620\n",
            "Epoch 26/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0065 - acc: 0.9980 - val_loss: 0.0140 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99620\n",
            "Epoch 27/100\n",
            "468/468 [==============================] - 16s 34ms/step - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0140 - val_acc: 0.9960\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99620\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 00027: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31be5df438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8VkzrI38IRH",
        "colab_type": "code",
        "outputId": "2af97c5e-3f3b-4770-9c79-c34eb05e3b50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_12.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 246us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01401617524717294, 0.9962]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv3jdtJp8IKr",
        "colab_type": "code",
        "outputId": "63742cfa-7f84-419a-b885-c74d14c1f295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1961
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization, concatenate\n",
        "\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "\n",
        "x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "x = Conv2D(32, (1, 1), activation='relu')(x)\n",
        "\n",
        "x1 = Conv2D(32, (1, 3), activation='relu')(inputs)\n",
        "x1 = Conv2D(32, (3, 1), activation='relu')(x1)\n",
        "\n",
        "x2 = Conv2D(32, (1, 1), activation='relu')(inputs)\n",
        "x2 = Conv2D(32, (3, 3), activation='relu')(x2)\n",
        "\n",
        "x = concatenate([x, x1, x2])\n",
        "x = BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "\n",
        "temp = x\n",
        "x = Conv2D(64, (3, 3), activation='relu')(temp)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = Conv2D(64, (1, 1), activation='relu')(x)\n",
        "\n",
        "x1 = Conv2D(64, (1, 3), activation='relu')(temp)\n",
        "x1 = Conv2D(64, (3, 1), activation='relu')(x1)\n",
        "x1 = Conv2D(64, (3, 3), activation='relu')(x1)\n",
        "\n",
        "x2 = Conv2D(64, (1, 1), activation='relu')(temp)\n",
        "x2 = Conv2D(64, (3, 3), activation='relu')(x2)\n",
        "x2 = Conv2D(64, (3, 3), activation='relu')(x2)\n",
        "\n",
        "\n",
        "x3 = Conv2D(64, (1, 1), activation='relu')(temp)\n",
        "x3 = Conv2D(64, (5, 5), activation='relu')(x3)\n",
        "\n",
        "x4 = Conv2D(64, (5, 5), activation='relu')(temp)\n",
        "x4 = Conv2D(64, (1, 1), activation='relu')(x4)\n",
        "\n",
        "x = concatenate([x, x1, x2, x3, x4])\n",
        "x = BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "\n",
        "temp = x\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(temp)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "\n",
        "x1 = Conv2D(128, (2, 2), padding='same', activation='relu')(temp)\n",
        "x1 = Conv2D(128, (2, 2), padding='same', activation='relu')(x1)\n",
        "x1 = Conv2D(128, (2, 2), padding='same', activation='relu')(x1)\n",
        "x1 = Conv2D(128, (2, 2), padding='same', activation='relu')(x1)\n",
        "x1 = Conv2D(128, (2, 2), padding='same', activation='relu')(x1)\n",
        "\n",
        "\n",
        "x2 = Conv2D(128, (3, 1), padding='same', activation='relu')(temp)\n",
        "x2 = Conv2D(128, (1, 3), padding='same', activation='relu')(x2)\n",
        "x2 = Conv2D(128, (3, 1), padding='same', activation='relu')(x2)\n",
        "x2 = Conv2D(128, (1, 3), padding='same', activation='relu')(x2)\n",
        "x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)\n",
        "\n",
        "x = concatenate([x, x1, x2, temp])\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "x = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 26, 26, 32)   320         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 28, 26, 32)   128         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 28, 28, 32)   64          input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 26, 26, 32)   1056        conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 26, 26, 32)   3104        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 26, 26, 32)   9248        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 26, 26, 96)   0           conv2d_85[0][0]                  \n",
            "                                                                 conv2d_87[0][0]                  \n",
            "                                                                 conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 26, 26, 96)   384         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling2D) (None, 13, 13, 96)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 11, 11, 64)   55360       max_pooling2d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 13, 11, 64)   18496       max_pooling2d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 13, 13, 64)   6208        max_pooling2d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 9, 9, 64)     36928       conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 11, 11, 64)   12352       conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 11, 11, 64)   36928       conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 13, 13, 64)   6208        max_pooling2d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 9, 9, 64)     153664      max_pooling2d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 9, 9, 64)     4160        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 9, 9, 64)     36928       conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 9, 9, 64)     36928       conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 9, 9, 64)     102464      conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 9, 9, 64)     4160        conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 9, 9, 320)    0           conv2d_92[0][0]                  \n",
            "                                                                 conv2d_95[0][0]                  \n",
            "                                                                 conv2d_98[0][0]                  \n",
            "                                                                 conv2d_100[0][0]                 \n",
            "                                                                 conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 9, 9, 320)    1280        concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling2D) (None, 4, 4, 320)    0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 4, 4, 128)    368768      max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 4, 4, 128)    163968      max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 4, 4, 128)    123008      max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 4, 4, 128)    147584      conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 4, 4, 128)    65664       conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 4, 4, 128)    49280       conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 4, 4, 128)    147584      conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 4, 4, 128)    65664       conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 4, 4, 128)    49280       conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 4, 4, 128)    147584      conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 4, 4, 128)    65664       conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 4, 4, 128)    49280       conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 4, 4, 128)    147584      conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 4, 4, 128)    65664       conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 4, 4, 128)    16512       conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 4, 4, 704)    0           conv2d_107[0][0]                 \n",
            "                                                                 conv2d_112[0][0]                 \n",
            "                                                                 conv2d_117[0][0]                 \n",
            "                                                                 max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 4, 4, 704)    2816        concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_8 (Glo (None, 704)          0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 704)          0           global_average_pooling2d_8[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 10)           7050        dropout_13[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 2,209,322\n",
            "Trainable params: 2,207,082\n",
            "Non-trainable params: 2,240\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7o2o7TL8ICi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # 在数据集上将输入平均值设置为0\n",
        "        samplewise_center=False,  # 将每个样本均值设置为0\n",
        "        featurewise_std_normalization=False,  # 将输入除以数据集的std\n",
        "        samplewise_std_normalization=False,  # 将每个输入除以其std\n",
        "        zca_whitening=False,  # 应用ZCA白化\n",
        "        rotation_range=10,  # 在一个范围下随机旋转图像(degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # 水平随机移位图像（总宽度的分数）\n",
        "        height_shift_range=0.1,  # 随机地垂直移动图像（总高度的分数）\n",
        "        horizontal_flip=False,  # 随机翻转图像\n",
        "        vertical_flip=False)  # 随机翻转图像\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkdgs6W-ADK9",
        "colab_type": "code",
        "outputId": "b3f851a3-8ab8-44c3-f957-8e9e7818827c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4501
        }
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_13.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=10, verbose=1, min_lr=1e-7)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                        batch_size=128),\n",
        "                        samples_per_epoch=x_train.shape[0],\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=468, epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "468/468 [==============================] - 30s 64ms/step - loss: 0.1375 - acc: 0.9583 - val_loss: 0.1052 - val_acc: 0.9724\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97240, saving model to mnist_model_13.h5\n",
            "Epoch 2/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0579 - acc: 0.9821 - val_loss: 0.0477 - val_acc: 0.9851\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97240 to 0.98510, saving model to mnist_model_13.h5\n",
            "Epoch 3/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0406 - acc: 0.9881 - val_loss: 0.0560 - val_acc: 0.9834\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.98510\n",
            "Epoch 4/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0369 - acc: 0.9889 - val_loss: 0.0326 - val_acc: 0.9888\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98510 to 0.98880, saving model to mnist_model_13.h5\n",
            "Epoch 5/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0328 - acc: 0.9901 - val_loss: 0.0362 - val_acc: 0.9886\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.98880\n",
            "Epoch 6/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0312 - acc: 0.9905 - val_loss: 0.0280 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98880 to 0.99150, saving model to mnist_model_13.h5\n",
            "Epoch 7/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0288 - acc: 0.9917 - val_loss: 0.0329 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99150\n",
            "Epoch 8/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0269 - acc: 0.9922 - val_loss: 0.0250 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.99150 to 0.99300, saving model to mnist_model_13.h5\n",
            "Epoch 9/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0255 - acc: 0.9924 - val_loss: 0.0374 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99300\n",
            "Epoch 10/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0228 - acc: 0.9935 - val_loss: 0.0212 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.99300 to 0.99350, saving model to mnist_model_13.h5\n",
            "Epoch 11/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0237 - acc: 0.9930 - val_loss: 0.0261 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99350\n",
            "Epoch 12/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0217 - acc: 0.9932 - val_loss: 0.0223 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99350\n",
            "Epoch 13/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0209 - acc: 0.9936 - val_loss: 0.0237 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99350\n",
            "Epoch 14/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0186 - acc: 0.9944 - val_loss: 0.0422 - val_acc: 0.9883\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99350\n",
            "Epoch 15/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0188 - acc: 0.9945 - val_loss: 0.0241 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99350\n",
            "Epoch 16/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.0252 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99350\n",
            "Epoch 17/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0162 - acc: 0.9950 - val_loss: 0.0237 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99350\n",
            "Epoch 18/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0173 - acc: 0.9946 - val_loss: 0.0261 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99350\n",
            "Epoch 19/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0158 - acc: 0.9956 - val_loss: 0.0495 - val_acc: 0.9872\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99350\n",
            "Epoch 20/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0150 - acc: 0.9957 - val_loss: 0.0297 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99350\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 21/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0121 - val_acc: 0.9962\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.99350 to 0.99620, saving model to mnist_model_13.h5\n",
            "Epoch 22/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0121 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.99620 to 0.99630, saving model to mnist_model_13.h5\n",
            "Epoch 23/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0110 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.99630 to 0.99640, saving model to mnist_model_13.h5\n",
            "Epoch 24/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0124 - val_acc: 0.9959\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99640\n",
            "Epoch 25/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0113 - val_acc: 0.9966\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.99640 to 0.99660, saving model to mnist_model_13.h5\n",
            "Epoch 26/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0117 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99660\n",
            "Epoch 27/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0116 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99660\n",
            "Epoch 28/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0179 - val_acc: 0.9958\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99660\n",
            "Epoch 29/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0040 - acc: 0.9986 - val_loss: 0.0174 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99660\n",
            "Epoch 30/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0149 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99660\n",
            "Epoch 31/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0184 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99660\n",
            "Epoch 32/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0132 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99660\n",
            "Epoch 33/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0136 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99660\n",
            "Epoch 34/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0163 - val_acc: 0.9958\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99660\n",
            "Epoch 35/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0158 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99660\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 36/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0137 - val_acc: 0.9968\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.99660 to 0.99680, saving model to mnist_model_13.h5\n",
            "Epoch 37/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0138 - val_acc: 0.9966\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99680\n",
            "Epoch 38/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0022 - acc: 0.9991 - val_loss: 0.0132 - val_acc: 0.9966\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99680\n",
            "Epoch 39/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0130 - val_acc: 0.9969\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.99680 to 0.99690, saving model to mnist_model_13.h5\n",
            "Epoch 40/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0132 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99690\n",
            "Epoch 41/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0136 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.99690\n",
            "Epoch 42/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0129 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.99690\n",
            "Epoch 43/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0132 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.99690\n",
            "Epoch 44/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0130 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.99690\n",
            "Epoch 45/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0136 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.99690\n",
            "Epoch 46/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0135 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.99690\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 47/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0134 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.99690\n",
            "Epoch 48/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0133 - val_acc: 0.9966\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.99690\n",
            "Epoch 49/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0132 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.99690\n",
            "Epoch 50/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0131 - val_acc: 0.9964\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.99690\n",
            "Epoch 51/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0132 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.99690\n",
            "Epoch 52/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0131 - val_acc: 0.9966\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.99690\n",
            "Epoch 53/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0131 - val_acc: 0.9966\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.99690\n",
            "Epoch 54/100\n",
            "468/468 [==============================] - 21s 46ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0127 - val_acc: 0.9966\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.99690\n",
            "Epoch 55/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0127 - val_acc: 0.9967\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.99690\n",
            "Epoch 56/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0128 - val_acc: 0.9967\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.99690\n",
            "\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 57/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0129 - val_acc: 0.9967\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.99690\n",
            "Epoch 58/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0129 - val_acc: 0.9967\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.99690\n",
            "Epoch 59/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0129 - val_acc: 0.9968\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.99690\n",
            "Epoch 00059: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31baf087b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irizE6YVADXL",
        "colab_type": "code",
        "outputId": "c7bb3249-74f9-4e69-f196-936fe8d926bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_13.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 367us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.012970936393890225, 0.9969]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP_twShUMPkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4CsLR2MoQdb",
        "colab_type": "text"
      },
      "source": [
        "### 循环神经网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bou0HQ7bpJV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxkUXYr6oU0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyJMZuWNo0PN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "8834285a-f67b-4a1e-8d1e-91adc52f462a"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import SimpleRNN, Activation, Dense, Input\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "#   https://keras.io/zh/layers/recurrent/\n",
        "\n",
        "model.add(SimpleRNN(256,     # <<------------------------\n",
        "    batch_input_shape=(None, 28, 28),\n",
        "))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax')) \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_5 (SimpleRNN)     (None, 256)               72960     \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 75,530\n",
            "Trainable params: 75,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po-ST9lJo0Jh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2951
        },
        "outputId": "5197c83f-8d9e-4c99-ee86-f20d17c93c4c"
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_14.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 21s 350us/step - loss: 0.1671 - acc: 0.9504 - val_loss: 0.1487 - val_acc: 0.9541\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.95410, saving model to mnist_model_14.h5\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.1300 - acc: 0.9613 - val_loss: 0.1197 - val_acc: 0.9649\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.95410 to 0.96490, saving model to mnist_model_14.h5\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.1114 - acc: 0.9675 - val_loss: 0.0924 - val_acc: 0.9721\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.96490 to 0.97210, saving model to mnist_model_14.h5\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.1043 - acc: 0.9688 - val_loss: 0.0825 - val_acc: 0.9753\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.97210 to 0.97530, saving model to mnist_model_14.h5\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0953 - acc: 0.9717 - val_loss: 0.1407 - val_acc: 0.9603\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.97530\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0845 - acc: 0.9749 - val_loss: 0.0889 - val_acc: 0.9726\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.97530\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0849 - acc: 0.9754 - val_loss: 0.1108 - val_acc: 0.9669\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.97530\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0847 - acc: 0.9750 - val_loss: 0.0779 - val_acc: 0.9774\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.97530 to 0.97740, saving model to mnist_model_14.h5\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.0792 - acc: 0.9768 - val_loss: 0.1023 - val_acc: 0.9704\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.97740\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0801 - acc: 0.9764 - val_loss: 0.1035 - val_acc: 0.9668\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.97740\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0767 - acc: 0.9777 - val_loss: 0.1029 - val_acc: 0.9700\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.97740\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0703 - acc: 0.9793 - val_loss: 0.0888 - val_acc: 0.9746\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.97740\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0806 - acc: 0.9764 - val_loss: 0.0835 - val_acc: 0.9755\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.97740\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0356 - acc: 0.9894 - val_loss: 0.0514 - val_acc: 0.9853\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.97740 to 0.98530, saving model to mnist_model_14.h5\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0237 - acc: 0.9933 - val_loss: 0.0486 - val_acc: 0.9862\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.98530 to 0.98620, saving model to mnist_model_14.h5\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0188 - acc: 0.9946 - val_loss: 0.0491 - val_acc: 0.9858\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.98620\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0158 - acc: 0.9956 - val_loss: 0.0477 - val_acc: 0.9864\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.98620 to 0.98640, saving model to mnist_model_14.h5\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0149 - acc: 0.9956 - val_loss: 0.0510 - val_acc: 0.9851\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.98640\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0122 - acc: 0.9966 - val_loss: 0.0538 - val_acc: 0.9842\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.98640\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0105 - acc: 0.9974 - val_loss: 0.0568 - val_acc: 0.9854\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.98640\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 9s 148us/step - loss: 0.0105 - acc: 0.9972 - val_loss: 0.0526 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.98640\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.0095 - acc: 0.9975 - val_loss: 0.0536 - val_acc: 0.9856\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.98640\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0488 - val_acc: 0.9868\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.98640 to 0.98680, saving model to mnist_model_14.h5\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0044 - acc: 0.9992 - val_loss: 0.0487 - val_acc: 0.9865\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.98680\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.0504 - val_acc: 0.9868\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.98680\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 0.0505 - val_acc: 0.9872\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.98680 to 0.98720, saving model to mnist_model_14.h5\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0033 - acc: 0.9995 - val_loss: 0.0513 - val_acc: 0.9865\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.98720\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0030 - acc: 0.9996 - val_loss: 0.0518 - val_acc: 0.9873\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.98720 to 0.98730, saving model to mnist_model_14.h5\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0028 - acc: 0.9997 - val_loss: 0.0528 - val_acc: 0.9863\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.98730\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 9s 153us/step - loss: 0.0026 - acc: 0.9997 - val_loss: 0.0532 - val_acc: 0.9862\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.98730\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0025 - acc: 0.9997 - val_loss: 0.0534 - val_acc: 0.9866\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.98730\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.0536 - val_acc: 0.9865\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.98730\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.0539 - val_acc: 0.9863\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.98730\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.0537 - val_acc: 0.9866\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.98730\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.0540 - val_acc: 0.9866\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.98730\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.0541 - val_acc: 0.9868\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.98730\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.0542 - val_acc: 0.9865\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.98730\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.0543 - val_acc: 0.9865\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.98730\n",
            "Epoch 00038: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31a740f940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OEgIyeQo0D5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d73c5186-d535-4bf4-8ac9-6fb42a48e15f"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_14.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 5s 526us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.051839204063778746, 0.9873]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAGjHAlyoz7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "820ca75c-a141-4a56-8ea6-f240e504f305"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import SimpleRNN, Activation, Dense, Input, GRU, LSTM, ConvLSTM2D, CuDNNGRU, CuDNNLSTM\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(256,     # CuDNNLSTM   # <<------------------------\n",
        "    batch_input_shape=(None, 28, 28),\n",
        "))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))  \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 256)               291840    \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 294,410\n",
            "Trainable params: 294,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg7LRGlyoz0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2395
        },
        "outputId": "61985553-6091-488f-9d4a-22db9ece581a"
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_15.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 34s 567us/step - loss: 0.4490 - acc: 0.8513 - val_loss: 0.1928 - val_acc: 0.9412\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.94120, saving model to mnist_model_15.h5\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.1294 - acc: 0.9604 - val_loss: 0.1439 - val_acc: 0.9542\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.94120 to 0.95420, saving model to mnist_model_15.h5\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0865 - acc: 0.9734 - val_loss: 0.0807 - val_acc: 0.9742\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.95420 to 0.97420, saving model to mnist_model_15.h5\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0635 - acc: 0.9799 - val_loss: 0.0652 - val_acc: 0.9791\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.97420 to 0.97910, saving model to mnist_model_15.h5\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0531 - acc: 0.9828 - val_loss: 0.0780 - val_acc: 0.9765\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.97910\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0408 - acc: 0.9874 - val_loss: 0.0495 - val_acc: 0.9850\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.97910 to 0.98500, saving model to mnist_model_15.h5\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 21s 356us/step - loss: 0.0376 - acc: 0.9884 - val_loss: 0.0439 - val_acc: 0.9866\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.98500 to 0.98660, saving model to mnist_model_15.h5\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0304 - acc: 0.9908 - val_loss: 0.0481 - val_acc: 0.9858\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98660\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0269 - acc: 0.9915 - val_loss: 0.0440 - val_acc: 0.9861\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98660\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0247 - acc: 0.9920 - val_loss: 0.0497 - val_acc: 0.9856\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98660\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0237 - acc: 0.9929 - val_loss: 0.0471 - val_acc: 0.9857\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.98660\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0214 - acc: 0.9933 - val_loss: 0.0432 - val_acc: 0.9865\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.98660\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0078 - acc: 0.9981 - val_loss: 0.0306 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.98660 to 0.99030, saving model to mnist_model_15.h5\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0319 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99030\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 0.0343 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99030\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 21s 356us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0337 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99030\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0343 - val_acc: 0.9901\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99030\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0382 - val_acc: 0.9901\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99030\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 21s 358us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0373 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.99030 to 0.99050, saving model to mnist_model_15.h5\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0375 - val_acc: 0.9907\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.99050 to 0.99070, saving model to mnist_model_15.h5\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 9.7920e-04 - acc: 0.9999 - val_loss: 0.0383 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99070\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 9.0562e-04 - acc: 0.9999 - val_loss: 0.0389 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99070\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 8.4716e-04 - acc: 0.9999 - val_loss: 0.0393 - val_acc: 0.9901\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99070\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 7.7383e-04 - acc: 0.9999 - val_loss: 0.0406 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99070\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 6.9957e-04 - acc: 0.9999 - val_loss: 0.0405 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99070\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 6.0483e-04 - acc: 0.9999 - val_loss: 0.0410 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99070\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 5.6691e-04 - acc: 0.9999 - val_loss: 0.0413 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99070\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 5.5160e-04 - acc: 0.9999 - val_loss: 0.0416 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99070\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 5.3708e-04 - acc: 0.9999 - val_loss: 0.0419 - val_acc: 0.9899\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99070\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 5.1765e-04 - acc: 0.9999 - val_loss: 0.0420 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99070\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 00030: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31a79e9358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zNQI9WTuuBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bffb0a6d-4fbc-49af-d68a-e950734820f5"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_15.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 6s 584us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03754683790774143, 0.9907]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Fo2B0zut5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "fc5194b6-fc41-4756-ff35-ac682bea0367"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import SimpleRNN, Activation, Dense, Input, GRU, LSTM, ConvLSTM2D, CuDNNGRU, CuDNNLSTM\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(GRU(256,                        # <<------------------------\n",
        "    batch_input_shape=(None, 28, 28),\n",
        "))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))  \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_2 (GRU)                  (None, 256)               218880    \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 221,450\n",
            "Trainable params: 221,450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY38BGUyuty3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2071
        },
        "outputId": "818f4d60-1275-448d-cb2f-99e4070c320d"
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_16.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.4920 - acc: 0.8341 - val_loss: 0.1465 - val_acc: 0.9571\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.95710, saving model to mnist_model_16.h5\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.1225 - acc: 0.9623 - val_loss: 0.0896 - val_acc: 0.9720\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.95710 to 0.97200, saving model to mnist_model_16.h5\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0773 - acc: 0.9765 - val_loss: 0.0761 - val_acc: 0.9760\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.97200 to 0.97600, saving model to mnist_model_16.h5\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0570 - acc: 0.9826 - val_loss: 0.0608 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.97600 to 0.98120, saving model to mnist_model_16.h5\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0442 - acc: 0.9866 - val_loss: 0.0477 - val_acc: 0.9838\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98120 to 0.98380, saving model to mnist_model_16.h5\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0368 - acc: 0.9888 - val_loss: 0.0458 - val_acc: 0.9858\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98380 to 0.98580, saving model to mnist_model_16.h5\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0315 - acc: 0.9902 - val_loss: 0.0390 - val_acc: 0.9882\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.98580 to 0.98820, saving model to mnist_model_16.h5\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0286 - acc: 0.9912 - val_loss: 0.0333 - val_acc: 0.9893\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.98820 to 0.98930, saving model to mnist_model_16.h5\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0247 - acc: 0.9923 - val_loss: 0.0525 - val_acc: 0.9840\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98930\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0219 - acc: 0.9929 - val_loss: 0.0378 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.98930 to 0.98970, saving model to mnist_model_16.h5\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0194 - acc: 0.9939 - val_loss: 0.0354 - val_acc: 0.9889\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.98970\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0162 - acc: 0.9943 - val_loss: 0.0325 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.98970 to 0.98980, saving model to mnist_model_16.h5\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.0161 - acc: 0.9948 - val_loss: 0.0434 - val_acc: 0.9880\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.98980\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0136 - acc: 0.9957 - val_loss: 0.0482 - val_acc: 0.9875\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.98980\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.0145 - acc: 0.9954 - val_loss: 0.0414 - val_acc: 0.9874\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.98980\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0274 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.98980 to 0.99230, saving model to mnist_model_16.h5\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0298 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99230\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0304 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99230\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0324 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99230\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0326 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99230\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 8.7595e-04 - acc: 0.9998 - val_loss: 0.0332 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99230\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 4.9101e-04 - acc: 0.9999 - val_loss: 0.0335 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99230\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 3.6084e-04 - acc: 1.0000 - val_loss: 0.0345 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99230\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 3.1886e-04 - acc: 1.0000 - val_loss: 0.0346 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99230\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 2.8653e-04 - acc: 1.0000 - val_loss: 0.0354 - val_acc: 0.9907\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99230\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 2.5024e-04 - acc: 1.0000 - val_loss: 0.0357 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99230\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 00026: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31a3353320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhqaWbPJutsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7e14a678-f5ff-462c-92cc-3d76127015f9"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_16.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 6s 616us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.027376496727514313, 0.9923]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgOmqqdiutle",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "369a59cd-9453-4039-9a61-696d7f287c47"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import SimpleRNN, Activation, Dense, Input, GRU, LSTM, ConvLSTM2D, CuDNNGRU, CuDNNLSTM\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(CuDNNGRU(256, return_sequences=True, batch_input_shape=(None, 28, 28),))\n",
        "model.add(CuDNNGRU(256))\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))  \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnngru_1 (CuDNNGRU)       (None, 28, 256)           219648    \n",
            "_________________________________________________________________\n",
            "cu_dnngru_2 (CuDNNGRU)       (None, 256)               394752    \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 616,970\n",
            "Trainable params: 616,970\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPyA4vlVutez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2647
        },
        "outputId": "6c9163c5-8a55-4bfc-b669-7811258bc7cc"
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_17.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.3508 - acc: 0.8831 - val_loss: 0.1041 - val_acc: 0.9690\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.96900, saving model to mnist_model_17.h5\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0791 - acc: 0.9753 - val_loss: 0.0667 - val_acc: 0.9783\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.96900 to 0.97830, saving model to mnist_model_17.h5\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0535 - acc: 0.9832 - val_loss: 0.0482 - val_acc: 0.9846\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.97830 to 0.98460, saving model to mnist_model_17.h5\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0411 - acc: 0.9869 - val_loss: 0.0425 - val_acc: 0.9867\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98460 to 0.98670, saving model to mnist_model_17.h5\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0315 - acc: 0.9902 - val_loss: 0.0450 - val_acc: 0.9857\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.98670\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0258 - acc: 0.9920 - val_loss: 0.0398 - val_acc: 0.9886\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98670 to 0.98860, saving model to mnist_model_17.h5\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0227 - acc: 0.9927 - val_loss: 0.0348 - val_acc: 0.9894\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.98860 to 0.98940, saving model to mnist_model_17.h5\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0195 - acc: 0.9937 - val_loss: 0.0348 - val_acc: 0.9893\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98940\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0167 - acc: 0.9944 - val_loss: 0.0345 - val_acc: 0.9904\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.98940 to 0.99040, saving model to mnist_model_17.h5\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0157 - acc: 0.9948 - val_loss: 0.0325 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.99040 to 0.99050, saving model to mnist_model_17.h5\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0161 - acc: 0.9949 - val_loss: 0.0395 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99050\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.0316 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99050\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0086 - acc: 0.9971 - val_loss: 0.0395 - val_acc: 0.9889\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99050\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0126 - acc: 0.9958 - val_loss: 0.0384 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99050\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0086 - acc: 0.9972 - val_loss: 0.0352 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.99050 to 0.99090, saving model to mnist_model_17.h5\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0299 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.99090 to 0.99250, saving model to mnist_model_17.h5\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0074 - acc: 0.9977 - val_loss: 0.0406 - val_acc: 0.9892\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99250\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0417 - val_acc: 0.9882\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99250\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0442 - val_acc: 0.9882\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99250\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0316 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99250\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0301 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99250\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0250 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.99250 to 0.99350, saving model to mnist_model_17.h5\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 3.4052e-04 - acc: 1.0000 - val_loss: 0.0255 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99350\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 2.3359e-04 - acc: 1.0000 - val_loss: 0.0257 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.99350 to 0.99360, saving model to mnist_model_17.h5\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 1.7768e-04 - acc: 1.0000 - val_loss: 0.0259 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99360\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 1.3838e-04 - acc: 1.0000 - val_loss: 0.0264 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99360\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 1.0818e-04 - acc: 1.0000 - val_loss: 0.0268 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99360\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 8.5361e-05 - acc: 1.0000 - val_loss: 0.0269 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99360\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 8.1244e-05 - acc: 1.0000 - val_loss: 0.0270 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99360\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 7.6691e-05 - acc: 1.0000 - val_loss: 0.0272 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99360\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 7.1847e-05 - acc: 1.0000 - val_loss: 0.0273 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99360\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 6.6490e-05 - acc: 1.0000 - val_loss: 0.0276 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99360\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 6.0958e-05 - acc: 1.0000 - val_loss: 0.0276 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99360\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 5.9715e-05 - acc: 1.0000 - val_loss: 0.0277 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99360\n",
            "Epoch 00034: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31934629e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_MO2K0vutX9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "499a7f3f-edd0-4193-c09e-b2d64da172e8"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_17.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 6s 607us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.025689770274973125, 0.9936]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FURJlcmutRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "30f3938b-eb0e-4f92-fd5d-e3aa9a528aa1"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import SimpleRNN, Embedding, Dense, Input, GRU, LSTM, ConvLSTM2D, CuDNNGRU, CuDNNLSTM\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(1000, 256, input_length=28*28))\n",
        "model.add(CuDNNGRU(256))\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))  \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 784, 256)          256000    \n",
            "_________________________________________________________________\n",
            "cu_dnngru_11 (CuDNNGRU)      (None, 256)               394752    \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 653,322\n",
            "Trainable params: 653,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXlix1oL16tN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5SPW5Oq1yDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train = x_train.reshape(60000, 28*28)\n",
        "x_test = x_test.reshape(10000, 28*28)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzdK2NC-utGJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "40de246e-a3b9-4bb8-f6aa-f9edc05559cc"
      },
      "source": [
        "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('mnist_model_18.h5', monitor='val_acc', verbose=1, \n",
        "                                   mode='auto', period=1,save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "# optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks = callbacks\n",
        "         )"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 2.3014 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.11350, saving model to mnist_model_18.h5\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 57s 943us/step - loss: 2.3012 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.11350\n",
            "Epoch 3/100\n",
            " 9600/60000 [===>..........................] - ETA: 44s - loss: 2.3009 - acc: 0.1128"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-2336414f8139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m           \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m          )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fza_nN_21YLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6a82edbe-59b8-4a26-c8c7-a5a9e95699bd"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('mnist_model_18.h5')\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 9s 878us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.3011319984436036, 0.1135]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgD9QFRQ1YDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHXjRp0s1X2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LgzdZrooKmJ",
        "colab_type": "text"
      },
      "source": [
        "## ensemble 集成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBCNnZT5MPeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "def ensemble_predictions(num_networks):\n",
        "    pred_labels = []\n",
        "    test_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    # For each neural network in the ensemble.\n",
        "    for i in num_networks:\n",
        "        try:\n",
        "            model = load_model('mnist_model_{}.h5'.format(i))\n",
        "        except:\n",
        "            print('mnist_model_{}.h5'.format(i), '读取失败！')\n",
        "            continue\n",
        "        \n",
        "        result = model.evaluate(x_train, y_train, batch_size=256)\n",
        "        test_acc = result[-1]         #<<------------- this's train_acc, not test_acc\n",
        "        test_accuracies.append(test_acc)       \n",
        "        \n",
        "        result = model.evaluate(x_test, y_test, batch_size=256)\n",
        "        val_acc = result[-1]\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Print status message.\n",
        "        print(\"Network: {0}, Accuracy on Validation-Set: {1:.8f}, Test-Set: {2:.8f}\".format(i, val_acc, test_acc))\n",
        "\n",
        "\n",
        "        \n",
        "        pred = model.predict(x_test)\n",
        "        pred_labels.append(pred)\n",
        "    \n",
        "    return np.array(pred_labels), np.array(test_accuracies), np.array(val_accuracies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW4N5zHV4vFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4d1e2298-4404-4489-87cb-d7995a6c6326"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (60000, 10), (10000, 28, 28, 1), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW4-P217OaLi",
        "colab_type": "code",
        "outputId": "236dc187-c519-4208-e609-536eb7797727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "pred_labels0, test_accuracies0, val_accuracies0 = ensemble_predictions(num_networks=[4,5,6,7,8,9,10,11,12,13])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 4s 60us/step\n",
            "10000/10000 [==============================] - 0s 20us/step\n",
            "Network: 4, Accuracy on Validation-Set: 0.99380000, Test-Set: 0.99868333\n",
            "60000/60000 [==============================] - 4s 65us/step\n",
            "10000/10000 [==============================] - 0s 21us/step\n",
            "Network: 5, Accuracy on Validation-Set: 0.99400000, Test-Set: 0.99880000\n",
            "60000/60000 [==============================] - 4s 64us/step\n",
            "10000/10000 [==============================] - 0s 20us/step\n",
            "Network: 6, Accuracy on Validation-Set: 0.99430000, Test-Set: 0.99881667\n",
            "60000/60000 [==============================] - 6s 108us/step\n",
            "10000/10000 [==============================] - 1s 60us/step\n",
            "Network: 7, Accuracy on Validation-Set: 0.99470000, Test-Set: 0.99913333\n",
            "60000/60000 [==============================] - 7s 110us/step\n",
            "10000/10000 [==============================] - 1s 60us/step\n",
            "Network: 8, Accuracy on Validation-Set: 0.99540000, Test-Set: 1.00000000\n",
            "60000/60000 [==============================] - 7s 113us/step\n",
            "10000/10000 [==============================] - 1s 61us/step\n",
            "Network: 9, Accuracy on Validation-Set: 0.99640000, Test-Set: 1.00000000\n",
            "60000/60000 [==============================] - 7s 117us/step\n",
            "10000/10000 [==============================] - 1s 60us/step\n",
            "Network: 10, Accuracy on Validation-Set: 0.99680000, Test-Set: 0.99916667\n",
            "60000/60000 [==============================] - 6s 96us/step\n",
            "10000/10000 [==============================] - 0s 34us/step\n",
            "Network: 11, Accuracy on Validation-Set: 0.99700000, Test-Set: 0.99881667\n",
            "60000/60000 [==============================] - 6s 108us/step\n",
            "10000/10000 [==============================] - 0s 41us/step\n",
            "Network: 12, Accuracy on Validation-Set: 0.99620000, Test-Set: 0.99855000\n",
            "60000/60000 [==============================] - 10s 165us/step\n",
            "10000/10000 [==============================] - 1s 90us/step\n",
            "Network: 13, Accuracy on Validation-Set: 0.99690000, Test-Set: 0.99961667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ7kPynQ5E3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3df5842d-9087-4442-e521-b0bdbea4d6b7"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "import numpy as np\n",
        "x_train = x_train.reshape(60000, 28*28)\n",
        "x_test = x_test.reshape(10000, 28*28)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 784), (60000, 10), (10000, 784), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV1X-AKn4env",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "e7caf882-e197-4453-e852-87e7824ccea4"
      },
      "source": [
        "pred_labels1, test_accuracies1, val_accuracies1 = ensemble_predictions(num_networks=[1, 2, 3]) # 18"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 7s 117us/step\n",
            "10000/10000 [==============================] - 0s 17us/step\n",
            "Network: 1, Accuracy on Validation-Set: 0.98470000, Test-Set: 0.99566667\n",
            "60000/60000 [==============================] - 7s 121us/step\n",
            "10000/10000 [==============================] - 0s 17us/step\n",
            "Network: 2, Accuracy on Validation-Set: 0.98230000, Test-Set: 0.99528333\n",
            "60000/60000 [==============================] - 7s 119us/step\n",
            "10000/10000 [==============================] - 0s 16us/step\n",
            "Network: 3, Accuracy on Validation-Set: 0.98500000, Test-Set: 0.99688333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxLEyJnL4efk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c2c39b3-4c33-4674-8504-6635e34dd6d0"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000, 10), (10000, 28, 28), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HWIUl7N5kF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "d742e885-0893-4a88-f864-db7a76835d80"
      },
      "source": [
        "pred_labels2, test_accuracies2, val_accuracies2 = ensemble_predictions(num_networks=[14, 15, 16, 17])"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 9s 150us/step\n",
            "10000/10000 [==============================] - 0s 45us/step\n",
            "Network: 14, Accuracy on Validation-Set: 0.98730000, Test-Set: 0.99973333\n",
            "60000/60000 [==============================] - 11s 188us/step\n",
            "10000/10000 [==============================] - 1s 77us/step\n",
            "Network: 15, Accuracy on Validation-Set: 0.99070000, Test-Set: 0.99990000\n",
            "60000/60000 [==============================] - 11s 181us/step\n",
            "10000/10000 [==============================] - 1s 70us/step\n",
            "Network: 16, Accuracy on Validation-Set: 0.99230000, Test-Set: 0.99945000\n",
            "60000/60000 [==============================] - 9s 143us/step\n",
            "10000/10000 [==============================] - 0s 30us/step\n",
            "Network: 17, Accuracy on Validation-Set: 0.99360000, Test-Set: 1.00000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jN1lYh65jzH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "de7db2bb-2c3e-4f77-8cfa-076adbaefb5d"
      },
      "source": [
        "pred_labels = np.concatenate((pred_labels0, pred_labels1, pred_labels2), axis=0)\n",
        "\n",
        "test_accuracies = np.concatenate((test_accuracies0, test_accuracies1, test_accuracies2), axis=0)\n",
        "\n",
        "val_accuracies = np.concatenate((val_accuracies0, val_accuracies1, val_accuracies1), axis=0)\n",
        "\n",
        "\n",
        "pred_labels.shape, test_accuracies.shape, val_accuracies.shape"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17, 10000, 10), (17,), (16,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoKTZcLf4eW7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "8d4063e3-d74b-4627-cc53-78ccae346f22"
      },
      "source": [
        "print(\"Mean test-set accuracy: {0:.8f}\".format(np.mean(test_accuracies)))\n",
        "print(\"Min test-set accuracy:  {0:.8f}\".format(np.min(test_accuracies)))\n",
        "print(\"Max test-set accuracy:  {0:.8f}\".format(np.max(test_accuracies)))\n",
        "\n",
        "print(\"Mean val-set accuracy: {0:.8f}\".format(np.mean(val_accuracies)))\n",
        "print(\"Min val-set accuracy:  {0:.8f}\".format(np.min(val_accuracies)))\n",
        "print(\"Max val-set accuracy:  {0:.8f}\".format(np.max(val_accuracies)))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean test-set accuracy: 0.99873529\n",
            "Min test-set accuracy:  0.99528333\n",
            "Max test-set accuracy:  1.00000000\n",
            "Mean val-set accuracy: 0.99121875\n",
            "Min val-set accuracy:  0.98230000\n",
            "Max val-set accuracy:  0.99700000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16CbHxa74eNp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c289b5c-d2ce-4179-c311-5aa81afeb8d4"
      },
      "source": [
        "ensemble_pred_labels = np.mean(pred_labels, axis=0)\n",
        "ensemble_pred_labels.shape"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csZVa8g24eEZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "54f445de-26ec-4cdf-a79e-5283458ba872"
      },
      "source": [
        "ensemble_cls_pred = np.argmax(ensemble_pred_labels, axis=1)\n",
        "ensemble_cls_pred.shape"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGT3Ya2w4d6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a647c0b2-2623-46c7-8f00-10c8d2c292bb"
      },
      "source": [
        "labels_correct = np.argmax(y_test, axis=1)\n",
        "\n",
        "ensemble_correct = (ensemble_cls_pred == labels_correct)\n",
        "ensemble_correct"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True, ...,  True,  True,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrg0WB5v4dxG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c3b04261-3ca8-4d42-955d-98216abc3aae"
      },
      "source": [
        "print(np.sum(ensemble_correct))\n",
        "print('ensemble_model acc=', np.sum(ensemble_correct)/labels_correct.shape[0])"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9973\n",
            "ensemble_model acc= 0.9973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFd0JY2d-BXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7935ce2d-b74b-442c-8e90-80d60bd24729"
      },
      "source": [
        "ensemble_incorrect = np.logical_not(ensemble_correct)\n",
        "\n",
        "print('ensemble_model wrong=', np.sum(ensemble_incorrect)/labels_correct.shape[0])"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ensemble_model wrong= 0.0027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSY_l_Ie-X3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ca053f33-b16c-4804-9c13-9b7380fdc360"
      },
      "source": [
        "best_net = np.argmax(val_accuracies)\n",
        "print(best_net)\n",
        "val_accuracies[best_net]"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlDHlKTt-zzF",
        "colab_type": "text"
      },
      "source": [
        "ensemble_CNN_model val_acc= 0.9975\n",
        "\n",
        "ensemble_model val_acc= 0.9973\n",
        "\n",
        "best_net val_acc = 0.997\n",
        "\n",
        "*That's all!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_aChsxK_gE2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1db240a3-3e91-4ec2-f9b2-98a25c0b653b"
      },
      "source": [
        "val_accuracies"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9938, 0.994 , 0.9943, 0.9947, 0.9954, 0.9964, 0.9968, 0.997 ,\n",
              "       0.9962, 0.9969, 0.9847, 0.9823, 0.985 , 0.9847, 0.9823, 0.985 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    }
  ]
}